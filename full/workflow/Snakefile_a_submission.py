#Setting up conda
#navigate to /work/users/k/e/kellybce/ovale1r/snakemake
#Load anaconda through installation or local version
#module load anaconda/2021.11
#set up conda environment
#conda create -c conda-forge -c bioconda -n snakemake snakemake
#conda activate snakemake
#conda config --set channel_priority strict
###then run the snakefile from the snakemake/directory (Snakefile is the in the workflow directory)
#snakemake --use-conda --cores 1 --jobs 1 -s workflow/Snakefile

#TODO:
#	determine origin of the Pf3D7 core genome bed file I got from karamoko
#	add bcftools normalize step before limiting to biallelic snps? Would left-align variants but otherwise might not be important if we then just exclude indels
#  should tajima's D calculation include missing sites? currently not using sites with any missingness
#  check hard filtering cutoffs
#  Examine whether pi is different when using the unfiltered set of orthologs instead of masked
#  Add falciparum coverage filtering step
#  Have nsl and tajima's D steps read in vcf without MAF filter to see if this changes, rare alleles may have selectio signatures
#  collapse gzipping into a single general rule working with ".recode.vcf" or by combining into previous steps, using temporary() for the intermediate recoded unzipped vcfs
#  generate coditional script to quality filter tag for ovale (and gzip), and just copy the falciparum files
#  make chromosome vcf subsets temporary
#  Check that input filepaths only contain input files; move intermediate files to other directories
# ensure rule filter_maf and filter_missing_allmaf now accept tandem repeat excluded vcfs as input
#  consider masking orthologs that overlap tandem repeats? or just allow us to ignore variants within those orthologs when considering variants


#needed input files:
#	vcf of variants called for sample(s) after aligning to a specific reference genome, formatted as "input/vcfs/{project}_{species}.vcf.gz"
#	the reference genome to which sample reads were aligned, formatted as "input/genomes/{species}.fasta". Of note, PowCR01 needed to have its 14 chromosomes renamed manually
#   index files for each reference genome, generated by "bwa-mem2 index", "samtools faidx", and "gatk CreateSequenceDictionary -R"
#	bed file of chromosome intervals for P. o. curtisi and P. o. wallikeri genomes to include (which excludes extrachromosomal contigs), formatted as "input/beds/{species}_chr.bed", also excludes Poc chr10 because of low quality of the contig
#	bed file of specific hypervariable gene family intervals for the P. o. curtisi and P. o. wallikeri genomes, formatted as "input/beds/{species}_genemask.bed
#	bed file for P. falciparum core genome to be included, formatted as "input/beds/{species}_core.bed"
#	Determined hard quality filtering thresholds for ovale variants, given lack of literature on genomics of this organism
#	Use soft quality filter or preapplied quality filter for falciparum variants. Samples from Pf6k use the VQSLOD score filter
#	List of pf-poc one-to-one orthologs in the pf genome, in the same order as the corresponding orthologs from poc, after subsetting and filtering using Snakefile_ortholog_prep.py
#	List of pf-poc one-to-one orthologs in the poc genome, in the same order as the corresponding orthologs from pf, after subsetting and filtering using Snakefile_ortholog_prep.py
#	A map file of old and new names of the PowCR01 chromosomes; the new names cannot contain "|" characters

configfile: "config/config.yaml"

###create list of sample names
samplenames = []
for i in open(config["input_lists"]+"sample_names.txt").readlines():
        samplenames.append(i.rstrip("\n"))

###for ortholog filtering later, we must create sets of pf and poc orthologs. Switched to list format
def parse_bed_file(bed_file):
	gene_list=[]
	for line in open(bed_file):
		name = line.split("\t")[0]
		chrom = line.split("\t")[1]
		start = line.split("\t")[2]
		stop = line.split("\t")[3]
		window = int(stop) - int(start) + 1
		gene_list.append([name, chrom, start, stop, window])
	return gene_list

#def parse_bed_file(bed_file):
#	gene_dict={}
#	for line in open(bed_file):
#		name = line.split("\t")[0]
#		chrom = line.split("\t")[1]
#		start = line.split("\t")[2]
#		stop = line.split("\t")[3]
#		window = int(stop) - int(start) + 1
#		gene_dict[name]= [chrom, start, stop, window]
#	return gene_dict

pf3d7_orthos_masked = parse_bed_file(config["output"]+ config["pf_orthos_masked"])
curtisigh01_orthos_masked = parse_bed_file(config["output"]+config["poc_orthos_masked"])
wallikericr01_orthos_masked = parse_bed_file(config["output"]+config["pow_orthos_masked"])

###for nsl calculation, we also need a list of chromosome names for each species
###nSL is calculated for haplotypes on each chromosome, so we must subset vcfs by chromosome. This function derives a list of chromosome names from the chr.bed files
def	chrom_names(bed_file):
	chr_dict={}
	count = 0
	for line in open(bed_file):
		if count > 0:
			chrom = line.split("\t")[0]
			start = line.split("\t")[1]
			chr_dict[chrom]= [start]
		count += 1
	return chr_dict

pf3d7_chr = chrom_names(config["input_beds"]+ "pf3d7_chr.bed")
#print(pf3d7_chr)
curtisigh01_chr = chrom_names(config["input_beds"]+ "curtisigh01_chr.bed")
#print(curtisigh01_chr)
wallikericr01_chr = chrom_names(config["input_beds"]+ "wallikericr01_chr.bed")
#print(wallikericr01_chr)


###Determines the final files to be output by the snakemake pipeline
rule all:
#Previous rules should use wildcards for the project and species, but this final rule should employ the actual names and terms for the final file to be created
	input:
		###vcf filtering###
		#files = config["input_vcfs"]+"ov1_wallikericr01.vcf.gz",
		#pofiles = expand(config["output"]+"masked_vcfs/ov1_{species}_chrmasked.recode.vcf", species = ["wallikericr01", "curtisigh01"]),
		#pffiles = config["output"]+"masked_vcfs/ov1_pf3d7_masked.recode.vcf",
		#files = expand(config["output"]+"masked_vcfs/ov1_{species}_{mixed}_masked.vcf.gz", species = ["wallikericr01", "curtisigh01"], mixed = ["only","andmixed"]),
		#iles = config["output"]+"masked_vcfs/ov1_pf3d7_only_masked.vcf.gz",
		#files = expand(config["output"]+"masked_biallelic_vcfs/ov1_{species}_masked_biallelic.vcf.gz", species = ["wallikericr01", "curtisigh01", "pf3d7"]),
		#files = expand(config["output"]+"filtered_vcfs/ov1_{species}_masked_biallelic_filtertag.vcf.gz", species = ["wallikericr01", "curtisigh01"]),
		#o_files= expand(config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz.tbi", species = ["wallikericr01", "curtisigh01"], mixed = ["only","andmixed"]),
		#f_files = config["output"]+"filtered_vcfs/{project}_pf3d7_only_masked_biallelic_filtertag.vcf.gz.tbi"
		pf_files = config["output"]+"filtered_vcfs/ov1_pf3d7_only_masked_biallelic_qfiltered.vcf.gz",
		#calculate data file and bed files of tandem repeats among each species
		poc_trf = expand(config["output"]+"trfs/curtisigh01_{chromosome}.fasta.2.7.7.80.10.50.500.dat", chromosome = curtisigh01_chr.keys()),
		pow_trf = expand(config["output"]+"trfs/wallikericr01_{chromosome}.fasta.2.7.7.80.10.50.500.dat", chromosome = wallikericr01_chr.keys()),
		pf_trf = expand(config["output"]+"trfs/pf3d7_{chromosome}.fasta.2.7.7.80.10.50.500.dat", chromosome = pf3d7_chr.keys()),
		po_files = expand(config["output"]+"variant_tables/qfiltered/ov1_{species}_{mixed}_qfiltered.table", species = ["wallikericr01", "curtisigh01"], mixed = ["speciescall"]),
		#files = expand(config["output"]+"analysis_vcfs/ov1_{species}.vcf.gz", species = ["wallikericr01", "curtisigh01", "pf3d7"]),
		#files = config["output"]+"analysis_vcfs/ov1_pf3d7.vcf.gz",
		###Overal genome-wide statistics
		overall_snpden = expand(config["output"]+"statistics/overall_snp_density/ov1_{species}_speciescall.snpden", species =  ["wallikericr01", "curtisigh01"]),
		pf_snpden = config["output"]+"statistics/overall_snp_density/ov1_pf3d7_only.snpden",
		intron_beds = expand(config["output"]+"beds/{species}_introns_sorted.bed", species =  ["wallikericr01", "curtisigh01"]),
		ovale_region_snp_density = expand(config["output"]+"statistics/region_snp_density/ov1_{species}_speciescall_snp_density.txt", species =  ["wallikericr01", "curtisigh01"]),
		pf_region_snp_density = config["output"]+"statistics/region_snp_density/ov1_pf3d7_only_snp_density.txt",
		###COI Calculation
		coi = expand(config["output"]+"statistics/coi/{species}_{mixed}/ov1_{species}_{mixed}_coi.txt", species = ["wallikericr01", "curtisigh01"], mixed = ["speciescall"]),
		coi_pf = config["output"]+"statistics/coi/pf3d7_only/ov1_pf3d7_only_coi.txt",
		###Generate vcfs of monoclonal samples only
		mono_files = expand(config["output"]+"sample_sets/ov1_{species}_{mixed}_monoclonal.vcf.gz", species = ["wallikericr01", "curtisigh01"], mixed = ["speciescall"]),
		mono_pf_file = config["output"]+"sample_sets/ov1_pf3d7_only_monoclonal.vcf.gz",
		###generate consensus sequences per sample
		consensus = expand(config["output"]+"consensus_seq/ov1_{species}_speciescall-{sample}.fasta", species = ["wallikericr01", "curtisigh01"], sample = samplenames),
		###Generate plink files for PCA
		po_plink = expand(config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_{species}_speciescall/ov1_{species}_speciescall.eigenvec", species = ["wallikericr01", "curtisigh01"]),
		pf_plink = config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_pf3d7_only/ov1_pf3d7_only.eigenvec",
		###Admixture
		po_clusters = expand(config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_{species}_speciescall/run1/ov1_{species}_speciescall.clusters{cluster}.log", species = ["wallikericr01", "curtisigh01"], cluster = ["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15"]),
		pf_clusters = expand(config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_pf3d7_only/run1/ov1_pf3d7_only.clusters{cluster}.log", cluster = ["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18"]),
		###ortholog subsetting, processing, and management
		#pfpi = expand(config["output"]+"orthologs/overall/ov1_pf3d7_only_ortholog_{masked}_pi.txt", masked = ["masked"]),
		#pocpi = expand(config["output"]+"orthologs/overall/ov1_curtisigh01_speciescall_ortholog_{masked}_pi.txt", masked = ["masked"]),
		#powpi = expand(config["output"]+"orthologs/overall/ov1_wallikericr01_speciescall_ortholog_{masked}_pi.txt", masked = ["masked"]),

		###Selection signals###
		#nomissvcf = expand(config["output"]+"sample_sets/ov1_{species}_monoclonal_nomissing.vcf.gz", species = ["wallikericr01", "curtisigh01", "pf3d7"]),
		#pfnsl = expand(config["output"]+"selection/ov1_pf3d7_only_nsl_{chromosome}.nsl.out", chromosome = pf3d7_chr.keys()),
		#pocnsl = expand(config["output"]+"selection/ov1_curtisigh01_speciescall_nsl_{chromosome}.nsl.out", chromosome = curtisigh01_chr.keys()),
		#pownsl = expand(config["output"]+"selection/ov1_wallikericr01_speciescall_nsl_{chromosome}.nsl.out", chromosome = wallikericr01_chr.keys()),
		#nsl_compile_poc = config["output"]+"selection/ov1_curtisigh01_speciescall_nsl_total.txt",
		#nsl_compile = expand(config["output"]+"selection/ov1_{species}_speciescall_nsl_total.txt", species = ["wallikericr01", "curtisigh01"]),
		#nsl_pf_compile = config["output"]+"selection/ov1_pf3d7_only_nsl_total.txt",
		### Tajima's D calculations
		tajimad = expand(config["output"]+"selection/ov1_{species}_speciescall.Tajima.D", species = ["wallikericr01", "curtisigh01"]),
		tajima_compiled = expand(config["output"]+"selection/ov1_{species}_speciescall_tajimad_{elements}.txt", species = ["wallikericr01", "curtisigh01"], elements = ["genes","exons"]),
		tajima_pf_compiled = expand(config["output"]+"selection/ov1_pf3d7_only_tajimad_{elements}.txt", elements = ["genes","exons"]),
		###Snakemake administrative###
		config = config["output"]+"pipeline/config.yaml",
		snakefile = config["output"]+"pipeline/Snakefile_analysis.py"

### Step _: filter vcfs to core genome only
###
###

###For samples aligned to the PocGH01 or PowCR01 genomes, this limits the SNPs to chromosomal contigs only
rule mask_Po_chr:
	input:
	#This step requires that the ovale species names both end in "01", while the falciparum species name does not. This causes this step to only be run on ovale samples
		vcf = config["output"]+"vcfs/{project}/{project}_{ovale}01_{mixed}.vcf.gz",
		bed = config["input_beds"]+"{ovale}01_chr.bed"
	params:
		prefix = config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked"
	output:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"vcftools --gzvcf {input.vcf} --out {params.prefix} --bed {input.bed} --recode --recode-INFO-all"

###Recodes default output of mask_Po_chr as gzipped vcf files
rule recode_Po_chrmask:
	input:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.recode.vcf"
	output:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.vcf.gz"	
	shell:
		"bgzip -c {input} > {output}"

###For samples aligned to PocGH01 or PowCR01, maks SNPs in specific hypervariable gene families shown in Rutledge 2017
rule mask_Po_variablegenes:
	input:
		vcf = config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.vcf.gz",
		bed = config["input_beds"]+"{ovale}01_genemask.bed"
	params:
		prefix = config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_masked"
	output:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_masked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"""vcftools --gzvcf {input.vcf} --out {params.prefix} --exclude-bed {input.bed} --recode --recode-INFO-all"""


###For samples aligned to Pf3D7, this limits SNPs to the standard Pf3D7 core genome (masking telomeres and some specific gene families/regions)
rule mask_Pf_acessory_genome:
	input: 
		vcf = config["output"]+"vcfs/{project}/{project}_pf3d7_{mixed}.vcf.gz",
		bed = config["input_beds"]+"pf3d7_core.bed"
	params:
		prefix = config["output"]+"masked_vcfs/{project}_pf3d7_{mixed}_masked"
	output: config["output"]+"masked_vcfs/{project}_pf3d7_{mixed}_masked.recode.vcf"
		#this command outputs "masked_vcfs/{project}_pf3d7_only_masked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"""vcftools --gzvcf {input.vcf} --out {params.prefix} --bed {input.bed} --recode --recode-INFO-all"""


###final recoding after masking for all species
rule recode_masked:
	input:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.recode.vcf"
	output:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"

### produces index (.tbi) files for all masked vcfs
rule index_masked_vcfs:
	input:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz"
	output:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input}"""



### Step _: quality filter sites based on QC scores, minor allele frequency, and individual-level missingness
###
###


###limits to biallelic SNPs
rule biallelic:
	input:
		vcf = config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz",
		index = config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz.tbi"
	output:
		config["output"]+"masked_biallelic_vcfs/{project}_{species}_{mixed}_masked_biallelic.vcf.gz"
	params:
		reference = config["input_genomes"]+"{species}.fasta"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants -R {params.reference}  -V {input.vcf} --select-type-to-include SNP --restrict-alleles-to BIALLELIC -O {output}"

rule table_for_hardfilter:
	input:
		config["output"]+"masked_biallelic_vcfs/{project}_{species}_{mixed}_masked_biallelic.vcf.gz"
	output:
		config["output"]+"variant_tables/unqfiltered/{project}_{species}_{mixed}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input} -F CHROM -F POS -F QD -F FS -F SOR -F MQ -F MQRankSum -F ReadPosRankSum -O {output}"


###adds a filter-tag to all SNPs who fail to pass the following hard quality thresholds
rule po_qualfiltertagger:
	input:
		vcf = config["output"]+"masked_biallelic_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic.vcf.gz",
		reference = config["input_genomes"]+"{ovale}01.fasta",
		#table is not needed but should be generated and evaluated before hard-filtering
		table = config["output"]+"variant_tables/unqfiltered/{project}_{ovale}01_{mixed}.table"
	output:
		config["output"]+"filtered_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic_filtertag.vcf"
	params:
		filters = config["gatk_hardfiltersnps"],
		java_opts = config["general_java_opts"]
	conda:
		"envs/biallelic.yaml"
	threads: 2
	shell:
		"""gatk --java-options "{params.java_opts}" VariantFiltration {params.filters} -R {input.reference} -V {input.vcf} -O {output}"""
	

###gzip-compresses output vcf from rule po_qualfiltertagger
rule po_qual_gzip:
	input:
		config["output"]+"filtered_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic_filtertag.vcf"
	output:
		config["output"]+"filtered_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic_filtertag.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"		

###adds a filter-tag to all SNPs who fail the VQSLOD quality score, a default metric provided by Pf6k
###This score and filter-tag are already included, so this rule just copies and indexes the same vcf file
rule pf_qualfiltertagger:
	input:
		vcf = config["output"]+"masked_biallelic_vcfs/{project}_pf3d7_{mixed}_masked_biallelic.vcf.gz",
		reference = config["input_genomes"]+"pf3d7.fasta"
	output:
		config["output"]+"filtered_vcfs/{project}_pf3d7_{mixed}_masked_biallelic_filtertag.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"""cp {input.vcf} {output}
		gatk IndexFeatureFile -I {output}"""

###produces index (.tbi) files for all qc-filter-tagged vcfs
rule index_tagged_vcfs:
		input:
			config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz"
		output:
			config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz.tbi"
		conda:
			"envs/masker.yaml"
		shell:
			"""bcftools index -t {input}"""
			
###Filter out all SNPs that fail QC (by hard filtering threshold for Po or the VQSLOD score for Pf) based on the presence of the filter tag
rule filter_by_tag:
	input: 
		vcf = config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz",
		index = config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz.tbi",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants -R {input.reference} -V {input.vcf} -O {output} --exclude-filtered"

rule eval_hard_filter:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz"
	output:
		config["output"]+"variant_tables/qfiltered/{project}_{species}_{mixed}_qfiltered.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input} -F CHROM -F POS -F QD -F FS -F SOR -F MQ -F MQRankSum -F ReadPosRankSum -O {output}"


###remove sites with tandem repeats
###first rule splits individual chromosomes 
rule split_poc_fasta_chr:
	input:
		ref = config["input_genomes"]+"curtisigh01.fasta"
	output:
		expand(config["output"] + "chr_fasta/curtisigh01_{chromosome}.fasta", chromosome = curtisigh01_chr.keys(), allow_missing = True),
	params:
		outfile = config["output"] + "chr_fasta/curtisigh01_"
	shell:
		"""csplit -s -z {input.ref} '/>/' '{{*}}'
		for i in xx* ; do \
		  n=$(sed 's/>// ; s/ .*// ; 1q' "$i") ; \
		  mv "$i" "{params.outfile}$n.fasta" ; \
		 done"""

rule split_pow_fasta_chr:
	input:
		ref = config["input_genomes"]+"wallikericr01.fasta"
	output:
		expand(config["output"] + "chr_fasta/wallikericr01_{chromosome}.fasta", chromosome = wallikericr01_chr.keys(), allow_missing = True),
	params:
		outfile = config["output"] + "chr_fasta/wallikericr01_"
	shell:
		"""csplit -s -z {input.ref} '/>/' '{{*}}'
		for i in xx* ; do \
		  n=$(sed 's/>// ; s/ .*// ; 1q' "$i") ; \
		  mv "$i" "{params.outfile}$n.fasta" ; \
		 done"""

rule split_pf_fasta_chr:
	input:
		ref = config["input_genomes"]+"pf3d7.fasta"
	output:
		expand(config["output"] + "chr_fasta/pf3d7_{chromosome}.fasta", chromosome = pf3d7_chr.keys(), allow_missing = True),
	params:
		outfile = config["output"] + "chr_fasta/pf3d7_"
	shell:
		"""csplit -s -z {input.ref} '/>/' '{{*}}'
		for i in xx* ; do \
		  n=$(sed 's/>// ; s/ .*// ; 1q' "$i") ; \
		  mv "$i" "{params.outfile}$n.fasta" ; \
		 done"""
	
rule tandem_repeat_finder:
	input:
		ref = config["output"]+"chr_fasta/{species}_{chromosome}.fasta",
	output:
		config["output"]+"trfs/{species}_{chromosome}.fasta.2.7.7.80.10.50.500.dat"
	params:
		outdir = config["output"]+"trfs/",
		rel_ref = "../../"+config["output"]+"chr_fasta/{species}_{chromosome}.fasta",
	conda:
		"envs/trf.yaml"
	shell:
		"""cd output/trfs/
		trf {params.relref} 2 7 7 80 10 50 500 -d -h"""
		
###second rule removes them
###must have output for maf filtering and non-maffiltering

###remove sites with a MAF lower than a threshold
rule filter_maf:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmaffiltered.vcf.gz"
	conda:
		"envs/filter.yaml"
	#### ADD a param to allow adjusting MAF, gauge whether MAF is suffieicnt for robustenss to sequencing error
	shell:
#the following command removes any sites with a minor allele frequency less than 5%
		"vcftools --gzvcf {input} --out {output} --recode --maf 0.05 --stdout | bgzip > {output}"

###only keep sites which are present (nonmissing) in XX% of samples
rule filter_missing:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmaffiltered.vcf.gz"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmafmissfiltered.vcf.gz"
	conda:
		"envs/filter.yaml"
	shell:
#This command limits the vcf to sites which were present in at least 80% of individuals
		"vcftools --gzvcf {input} --out {output} --max-missing 0.8 --recode --stdout | bgzip > {output}"

###Creates final cleaned vcf for downstream analysis
rule final_set:
#moves annotated processed vcfs to a new directory for further analysis
#vcfs in the analysis_vcfs directory have been limited to the core genome (in ovale, this means chromosomal contigs with a specific list 
# of hypervariable genes manually masked based on Rutledge 2017; limited to biallelic snps; and quality filtered by hard thresholds (in ovale) or the 
# default VQSLOD filter applied in the Pf6k dataset (for falciparum), minor allele frequency of 5%, and presence in >80% of samples 
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmafmissfiltered.vcf.gz"
	output:
		config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	shell:
		"cp {input} {output}"

rule final_table:
	input:
		config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	output:
		config["output"]+"analysis_tables/{project}_{species}_{mixed}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input} -F CHROM -F POS -F QD -F FS -F SOR -F MQ -F MQRankSum -F ReadPosRankSum -O {output}"


rule index_final_vcfs:
	input:
		vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		table = config["output"]+"analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""



### Create final vcfs without minor allele frequency filter
###only keep sites which are present (nonmissing) in XX% of samples
rule filter_missing_allmaf:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmissfiltered.vcf.gz"
	conda:
		"envs/filter.yaml"
	shell:
#This command limits the vcf to sites which were present in at least 80% of individuals
		"vcftools --gzvcf {input} --out {output} --max-missing 0.8 --recode --stdout | bgzip > {output}"

###Creates final cleaned vcf for downstream analysis
rule final_set_allmaf:
#moves annotated processed vcfs to a new directory for further analysis
#vcfs in the analysis_vcfs directory have been limited to the core genome (in ovale, this means chromosomal contigs with a specific list 
# of hypervariable genes manually masked based on Rutledge 2017; limited to biallelic snps; and quality filtered by hard thresholds (in ovale) or the 
# default VQSLOD filter applied in the Pf6k dataset (for falciparum), and presence in >80% of samples 
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmissfiltered.vcf.gz"
	output:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	shell:
		"cp {input} {output}"


rule index_final_vcfs_allmaf:
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

rule final_table_allmaf:
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi"
	output:
		config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_genome.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input.vcf} -F CHROM -F POS -O {output}"



### Determine SNP density across the genome and within specific functional portions of the genome


rule overall_snp_density_vcftools:
#calculates genome-wide SNP density per 1Kb
	input:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
		#config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	output:
		config["output"]+"statistics/overall_snp_density/{project}_{species}_{mixed}.snpden"
	params:
		outfile = config["output"]+"statistics/overall_snp_density/{project}_{species}_{mixed}"
	conda:
		"envs/filter.yaml"
	shell:
		"""vcftools --gzvcf {input} --SNPdensity 1000 --out {params.outfile}"""

###To calculate SNP Density at various regions of the genome, we must import bed files of genes, exons, CDS from the gff file
### a bed file of chromosome sizes can be produced from the chromosome bed file using : cat curtisigh01_chr.bed | while read CHROM START STOP; do echo -e "$CHROM\t$STOP" >> curtisigh01_chrsize.bed; done

rule chromosome_size_bed:
	input:
		config["input_beds"]+ "{species}.bed"
	output:
		config["output"]+ "beds/{species}_chrsize.txt"
	shell:
		'''cat {input} | while read CHROM START STOP; do echo -e "$CHROM\t$STOP" | sed 's/\r$//' >> {output}; done'''


rule sort_gffs:
	input:
		config["input"]+"gffs/{species}.gff"
	output:
		config["output"]+"gffs/{species}_sorted.gff"
	shell:
		'''cat {input} | awk '$1 ~ /^#/ {{print $0;next}} {{print $0 | "sort -k1,1 -k4,4n -k5,5n"}}' > {output}'''

rule intergenic_bed:
	input:
		gff_sorted = config["output"]+"gffs/{species}_sorted.gff",
		chrsize_bed = config["output"]+ "beds/{species}_chrsize.txt"
	output:
		config["output"]+"beds/{species}_intergenic_sorted.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		'''bedtools complement -i {input.gff_sorted} -g {input.chrsize_bed} > {output}'''

rule exon_bed:
	input:
		config["output"]+"gffs/{species}_sorted.gff"
	output:
		config["output"]+ "beds/{species}_exons_sorted.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 == "exon") print $1, $4-1, $5}}' {input} > {output}'''

rule gene_bed:
	input:
		config["output"]+"gffs/{species}_sorted.gff"
	output:
		config["output"]+ "beds/{species}_genes_sorted.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 ~ "gene") print $1, $4-1, $5}}' {input} > {output}'''

rule cds_bed:
	input:
		config["output"]+"gffs/{species}_sorted.gff"
	output:
		config["output"]+ "beds/{species}_cds_sorted.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 == "CDS") print $1, $4-1, $5}}' {input} > {output}'''

rule intron_bed:
	input:
		exons = config["output"]+ "beds/{species}_exons_sorted.bed",
		intergenic = config["output"]+"beds/{species}_intergenic_sorted.bed",
		chrsize_bed = config["output"]+ "beds/{species}_chrsize.txt"
	output:
		config["output"]+"beds/{species}_introns_sorted.bed"
	shell:
		'''bedtools complement -i <(cat {input.exons} {input.intergenic} | sort -k1,1 -k2,2n) -g {input.chrsize_bed} > {output}'''

###for introns, exons, cds, genes, and intergenic region beds, need to remove extrachromosomal contigs and have sorted
rule remove_extrachromosomal:
	input:
		region_bed = config["output"]+ "beds/{species}_{region}_sorted.bed",
		chr_bed = config["input_beds"]+"{species}_chr.bed",
	output:
		config["output"]+ "beds/{species}_{region}_sorted_chrselected.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools multiinter -i {input.region_bed} {input.chr_bed} | awk '$4==2' > {output}"

rule genemask_ovale_region_beds:
#for specific  genome region bed files, removes the intervals masked by the hypervariable gene mask
	input:
		region_bed = config["output"]+ "beds/{ovale}01_{region}_sorted_chrselected.bed",
		genemask_bed = config["input_beds"]+"{ovale}01_genemask.bed",
	output:
		config["output"]+ "beds/{ovale}01_{region}_sorted_chrselected_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.genemask_bed} > {output}"

rule corelimit_pf3d7_region_beds:
	input:
		region_bed = config["output"]+ "beds/pf3d7_{region}_sorted.bed",
		core_bed = config["input_beds"]+"pf3d7_core.bed",
	output:
		config["output"]+ "beds/pf3d7_{region}_sorted_chrselected_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools multiinter -i {input.region_bed} {input.core_bed} | awk '$4==2' > {output}"

rule genemask_ovale_chr_bed:
	input:
		region_bed = config["input_beds"]+ "{ovale}01_chr.bed",
		genemask_bed = config["input_beds"]+"{ovale}01_genemask.bed",
	output:
		config["output"]+ "beds/{ovale}01_chr_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.genemask_bed} > {output}"

rule corelimit_pf3d7_chr_bed:
	input:
		region_bed = config["input_beds"]+ "pf3d7_chr.bed",
		core_bed = config["input_beds"]+"pf3d7_core.bed",
	output:
		config["output"]+ "beds/pf3d7_chr_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools multiinter -i {input.region_bed} {input.core_bed} | awk '$4==2' > {output}"

rule region_snp_table:
	input:
		#vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		#index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		bed = config["output"]+ "beds/{species}_{region}_sorted_chrselected_genemasked.bed"
	output:
		#config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_{region}.table"
		config["output"]+"region_tables/{project}_{species}_{mixed}_{region}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input.vcf} -L {input.bed} -F CHROM -F POS -O {output}"

rule merge_overlap:
#merge bed file windows so that overlapping genome elements do not falsely inflate region length for SNP density calculations (ie two overlapping exons from different genes shouldn't be counted as separate regions of the genome)
	input:
		config["output"]+ "beds/{species}_{region}_sorted_chrselected_genemasked.bed"
	output:
		config["output"]+ "beds/{species}_{region}_sorted_chrselected_genemasked_merged.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools merge -i {input} > {output}"				

###for each type of region, make a script that counts SNPs within the SNPs within the regions and divides by the total legnth of intervals in each bed file
# first, do a gatk varianttotable step using -L with the corresponding bed files
# then, take in the table and the bed file and do a simple calculation of numbers of rows in table divided by total length of intervals in bed file
rule compile_snp_density:
	input:
		genome_bed = config["output"]+ "beds/{species}_chr_genemasked.bed",
		intergenic_bed = config["output"]+"beds/{species}_intergenic_sorted_chrselected_genemasked_merged.bed",
		gene_bed = config["output"]+ "beds/{species}_genes_sorted_chrselected_genemasked_merged.bed",
		intron_bed =  config["output"]+"beds/{species}_introns_sorted_chrselected_genemasked_merged.bed",
		exon_bed = config["output"]+ "beds/{species}_exons_sorted_chrselected_genemasked_merged.bed",
		cds_bed = config["output"]+ "beds/{species}_cds_sorted_chrselected_genemasked_merged.bed",
		#genome_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_genome.table",
		#intergenic_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_intergenic.table",
		#gene_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_genes.table",
		#intron_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_introns.table",
		#exon_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_exons.table",
		#cds_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_cds.table",
		genome_table = config["output"]+"region_tables/{project}_{species}_{mixed}_genome.table",
		intergenic_table = config["output"]+"region_tables/{project}_{species}_{mixed}_intergenic.table",
		gene_table = config["output"]+"region_tables/{project}_{species}_{mixed}_genes.table",
		intron_table = config["output"]+"region_tables/{project}_{species}_{mixed}_introns.table",
		exon_table = config["output"]+"region_tables/{project}_{species}_{mixed}_exons.table",
		cds_table = config["output"]+"region_tables/{project}_{species}_{mixed}_cds.table",
	output:
		config["output"]+"statistics/region_snp_density/{project}_{species}_{mixed}_snp_density.txt"
	script:
		"scripts/region_snp_density_compiler.py"



### Section ___: generate outfiles for calculation of Complexity of Infection using THEREALMcCOIL


#### Trying with and without MAF filter
rule submit_mccoilr_script:
	input:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	output:
		config["output"]+"statistics/coi/{species}_{mixed}/{project}_{species}_{mixed}_coi.txt"
	conda:
		"envs/r.yaml"
	params:
		maxcoi = config["realmccoilr"]["maxcoi"],
		threshold_ind = config["realmccoilr"]["threshold_ind"],
		threshold_site = config["realmccoilr"]["threshold_site"],
		totalrun = config["realmccoilr"]["totalrun"],
		burnin = config["realmccoilr"]["burnin"],
	resources:
		mem_mb = 300000
	script:	
		"scripts/Complexity_of_infection.R"


###Based on summary from REALMcCOIL, generate polyclonal input sample lists 


rule subset_monoclonal:
###excludes polyclonal samples, as determined using RealMcCOIl
	input:
		vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		samplelist = config["input_lists"] + "{project}_polyclonalsamples.args",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --exclude-sample-name {input.samplelist} -R {input.reference} -V {input.vcf} -O {output}"


rule index_monoclonal_vcfs:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

rule subset_monoclonal_allmaf:
###excludes polyclonal samples, as determined using RealMcCOIl
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		samplelist = config["input_lists"] + "{project}_polyclonalsamples.args",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --exclude-sample-name {input.samplelist} -R {input.reference} -V {input.vcf} -O {output}"

rule index_monoclonal_vcfs_allmaf:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

###Transfers a copy of the config file and Snakefile to the output for future reference of those results
rule copy_snakefileandconfig:
	input:
		config = "config/config.yaml",
		snakefile = "workflow/Snakefile_analysis.py"
	output:
		config = config["output"]+"pipeline/config.yaml",
		snakefile = config["output"]+"pipeline/Snakefile_analysis.py"
	shell:
		"""cp {input.config} {output.config}
		cp {input.snakefile} {output.snakefile}"""


rule vcf_2_fasta:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz",
		index = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz.tbi",
		ref = config["input_genomes"]+"{species}.fasta"
	output: 
		config["output"]+"consensus_seq/{project}_{species}_{mixed}-{sample}.fasta"
	conda:
		"envs/bcftools.yaml"
	shell:
		#-I argument causes consensus sequences to use IUPAC codes for ambiguous base calls
		"bcftools consensus -I -f {input.ref} -s {wildcards.sample} -o {output} {input.vcf}"









### Perform Principal Components Analysis using plink
rule plinker:
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz"
	output:
		#directory(config["output"]+"plink/monoclonal/{project}_{species}_{mixed}")
		config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed"
	params:
		outfile = config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}"
	conda:
		"envs/plink.yaml"
	shell:
	##--vcf-half-call haploid means that half-missing sites will be treated as haploid/homozygous, rather than missing or reference
	##--vcf-idspace-to "_" replaces spaces in sample ID names with underscores
	##--set-missing-var-ids gives new variant ID for each locus comprised of @:# -> chrom:baseposition
		'''plink --vcf {input} --make-bed --keep-allele-order --set-missing-var-ids @:# --out {params.outfile} --double-id --vcf-half-call haploid --allow-extra-chr --vcf-idspace-to "_"'''

rule ld_mark_plink:
	input:
		config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed"
	output:
		config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.prune.in"
	params:
		outfile = config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		infile = config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		window = config["plink"]["window"],
		step = config["plink"]["step"],
		r2 = config["plink"]["r2"],
	conda:
		"envs/plink.yaml"	
	shell:
		'''plink --bfile {params.infile} --indep-pairwise {params.window} {params.step} {params.r2} --allow-extra-chr --out {params.outfile}'''


rule ld_filter_prune:
	input:
		config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.prune.in"
	output:
		bed = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed",
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bim"
	params:
		infile = config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		selectfile = config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.prune.in",
		outfile = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}"
	conda:
		"envs/plink.yaml"
	shell:
		'''plink --bfile {params.infile} --extract {params.selectfile} --allow-extra-chr --make-bed --out {params.outfile}'''

rule pca_plink:
	input:
		config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed"
	output:
		eigen = config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.eigenvec",
	params:
		infile = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		outfile = config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}"
	conda:
		"envs/plink.yaml"
	shell:
		'plink --bfile {params.infile} --pca "var-wts" header --allow-extra-chr --out {params.outfile}'

rule bim_alter_admixture:
###replaces non-human chromosome names so this can be read by ADMIXTURE
	input:
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bim",
	output:
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.origchrom.bim"
	params:
		tempbim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.temp.bim"
	conda:
		"envs/admixture.yaml"
	shell:
		"""cp {input.bim} {output.bim}
		awk '{{$1="0";print $0}}' {input.bim} > {params.tempbim}
		mv {params.tempbim} {input.bim}"""

rule admixture:
###for specified a priori cluster counts, assigns samples to clusters and reports cross-calidation error 10 times for each cluster count
### cross-validation error can be checked with the following bash code:
###for n in {min_cluster..max_cluster}; do for i in run*/*clusters${n}.log; do cat $i | grep "CV"; done; done
	input:
		bed = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed",
		#origchrom.bim only needed to ensure rule bim_alter_admixture was run previously
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.origchrom.bim"
	output:
		config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/run1/{project}_{species}_{mixed}.clusters{cluster}.log"
	params:
		cluster = lambda wildcards, output: wildcards.cluster,
		workdir = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/",
		relout = "{project}_{species}_{mixed}.clusters{cluster}.log",
		relbed = "../{project}_{species}_{mixed}.bed"
	conda:
		"envs/admixture.yaml"
	shell:
		"""cd {params.workdir};
		for k in {{1..10}}; do mkdir -p run${{k}}; cd run${{k}}; admixture --cv {params.relbed} {params.cluster} > {params.relout}; cd ..; done
		cd ../../../..;
		echo $PWD"""
	


### Section ___: Calculate nucleotide diversity (pi) for 1-to-1-to-1 orthologs of PocGH01, PowCR01, and Pf3D7

#rule calculate_poc_ortho_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
#	input:
#		povcf = config["output"]+"sample_sets/{project}_curtisigh01_{mixed}_monoclonal.vcf.gz",
#		poc_orthos_masked = config["input_beds"]+"curtisigh01_pf-poc-pow_orthologs_{masked}.tsv"
#	output:
#		popi = config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_{masked}.windowed.pi"
#	params:
#		pooutfile = config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_{masked}",
#		chrom = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[0],
#		start = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[1],
#		stop = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[2],
#		window = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[3]
#	conda:
#		"envs/filter.yaml"
#	resources:
#		mem_mb = 20
#	shell:
#		"""vcftools --gzvcf {input.povcf} --out {params.pooutfile}  --chr {params.chrom} --from-bp {params.start} --to-bp {params.stop} --window-pi {params.window} --window-pi-step 1"""

rule unthreaded_calculate_poc_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_curtisigh01_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_masked",pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing =True),
		ortholist = curtisigh01_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"

rule unthreaded_calculate_pow_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_wallikericr01_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_masked",pogeneid = list(i[0] for i in wallikericr01_orthos_masked), allow_missing =True),
		ortholist = wallikericr01_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"

rule unthreaded_calculate_pf_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_pf3d7_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{geneid}_masked.windowed.pi", geneid = list(i[0] for i in pf3d7_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{geneid}_masked",geneid = list(i[0] for i in pf3d7_orthos_masked), allow_missing =True),
		ortholist = pf3d7_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"


#rule calculate_pow_ortho_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
#	input:
#		povcf = config["output"]+"sample_sets/{project}_wallikericr01_{mixed}_monoclonal.vcf.gz",
#		pow_orthos_masked = config["input_beds"]+"wallikericr01_pf-poc-pow_orthologs_{masked}.tsv"
#	output:
#		popi = config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_{masked}.windowed.pi"
#	params:
#		pooutfile = config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_{masked}",
#		chrom = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[0],
#		start = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[1],
#		stop = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[2],
#		window = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[3]
#	conda:
#		"envs/filter.yaml"
#	resources:
#		mem_mb = 20
#	shell:
#		"""vcftools --gzvcf {input.povcf} --out {params.pooutfile}  --chr {params.chrom} --from-bp {params.start} --to-bp {params.stop} --window-pi {params.window} --window-pi-step 1"""

	
#rule calculate_pf_ortho_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
#	input:
#		pfvcf = config["output"]+"sample_sets/{project}_pf3d7_{mixed}_monoclonal.vcf.gz",
#		pf_orthos_masked = config["input_beds"]+"pf3d7_pf-poc-pow_orthologs_{masked}.tsv"
#	output:
#		pfpi = config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{pfgeneid}_{masked}.windowed.pi"
#	params:
#		pfoutfile = config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{pfgeneid}_{masked}",
#		chrom = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[0],
#		start = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[1],
#		stop = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[2],
#		window = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[3]
#	conda:
#		"envs/filter.yaml"
#	resources:
#		mem_mb = 20
#	shell:
#		"""vcftools --gzvcf {input.pfvcf} --out {params.pfoutfile}  --chr {params.chrom} --from-bp {params.start} --to-bp {params.stop} --window-pi {params.window} --window-pi-step 1"""
#	
#rule select_pfpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
#	input:
#		pfpi = config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{pfgeneid}_{masked}.windowed.pi"
#	output:
#		pfpitable = config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{pfgeneid}_{masked}_pi.txt"
#	params:
#		chrom = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[0],
#		start = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[1],
#		stop = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[2],
#		window = lambda wildcards, output: list(pf3d7_orthos[wildcards.pfgeneid])[3]
#	resources:
#		mem_mb = 20
#	shell:
#		'''if cat {input.pfpi} | while read CHROM START END N_VARIANTS PI; do echo "{wildcards.pfgeneid} $CHROM $START $END $N_VARIANTS $PI"; done | grep "{params.start} {params.stop}"; then
#		cat {input.pfpi} | while read CHROM START END N_VARIANTS PI; do echo "{wildcards.pfgeneid} $CHROM $START $END $N_VARIANTS $PI"; done | grep "{params.start} {params.stop}" >> {output.pfpitable};
#		else echo "{wildcards.pfgeneid} {params.chrom} {params.start} {params.stop} 0 0" >> {output.pfpitable};fi'''

rule unthreaded_select_pocpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_masked_pi.txt", pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing =True)
	params:
		#inputpi = list(expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing = True)),
		geneid = list(i[0] for i in curtisigh01_orthos_masked),
		chrom = list(i[1] for i in curtisigh01_orthos_masked),
		start = list(i[2] for i in curtisigh01_orthos_masked),
		stop = list(i[3] for i in curtisigh01_orthos_masked),
		window = list(i[4] for i in curtisigh01_orthos_masked),
		#n_index = len(curtisigh01_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
				cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
				else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''

rule unthreaded_select_powpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_masked_pi.txt", pogeneid = list(i[0] for i in wallikericr01_orthos_masked), allow_missing =True)
	params:
		inputpi = list(expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_orthos_masked), allow_missing =True)),
		geneid = list(i[0] for i in wallikericr01_orthos_masked),
		chrom = list(i[1] for i in wallikericr01_orthos_masked),
		start = list(i[2] for i in wallikericr01_orthos_masked),
		stop = list(i[3] for i in wallikericr01_orthos_masked),
		window = list(i[4] for i in wallikericr01_orthos_masked),
		#n_index = len(wallikericr01_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		echo ${{!geneid[@]}};
		echo ${{!chrom[@]}};
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
			cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
			else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''

rule unthreaded_select_pfpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{geneid}_masked.windowed.pi", geneid = list(i[0] for i in pf3d7_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{geneid}_masked_pi.txt", geneid = list(i[0] for i in pf3d7_orthos_masked), allow_missing =True)
	params:
		inputpi = list(expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{geneid}_masked.windowed.pi", geneid = list(i[0] for i in pf3d7_orthos_masked), allow_missing =True)),
		geneid = list(i[0] for i in pf3d7_orthos_masked),
		chrom = list(i[1] for i in pf3d7_orthos_masked),
		start = list(i[2] for i in pf3d7_orthos_masked),
		stop = list(i[3] for i in pf3d7_orthos_masked),
		window = list(i[4] for i in pf3d7_orthos_masked),
		#n_index = len(pf3d7_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
				cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
				else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''



#rule select_pocpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
#	input:
#		popi = config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_{masked}.windowed.pi"
#	output:
#		popitable = config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pogeneid}_{masked}_pi.txt"
#	params:
#		chrom = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[0],
#		start = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[1],
#		stop = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[2],
#		window = lambda wildcards, output: list(curtisigh01_orthos[wildcards.pogeneid])[3]
#	resources:
#		mem_mb = 20
#	shell:
#		'''if cat {input.popi} | while read CHROM START END N_VARIANTS PI; do echo "{wildcards.pogeneid} $CHROM $START $END $N_VARIANTS $PI"; done | grep "{params.start} {params.stop}"; then
#				cat {input.popi} | while read CHROM START END N_VARIANTS PI; do echo "{wildcards.pogeneid} $CHROM $START $END $N_VARIANTS $PI"; done | grep "{params.start} {params.stop}" >> {output.popitable};
#				else echo "{wildcards.pogeneid} {params.chrom} {params.start} {params.stop} 0 0" >> {output.popitable};fi'''

#rule select_powpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
#	input:
#		popi = config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_{masked}.windowed.pi"
#	output:
#		popitable = config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{pogeneid}_{masked}_pi.txt"
#	params:
#		chrom = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[0],
#		start = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[1],
#		stop = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[2],
#		window = lambda wildcards, output: list(wallikericr01_orthos[wildcards.pogeneid])[3]
#	resources:
#		mem_mb = 20
#	shell:
#		'''if cat {input.popi} | while read CHROM START END N_VARIANTS PI; do echo "{wildcards.pogeneid} $CHROM $START $END $N_VARIANTS $PI"; done | grep "{params.start} {params.stop}"; then
#		cat {input.popi} | while read CHROM START END N_VARIANTS PI; do echo "{wildcards.pogeneid} $CHROM $START $END $N_VARIANTS $PI"; done | grep "{params.start} {params.stop}" >> {output.popitable};
#		else echo "{wildcards.pogeneid} {params.chrom} {params.start} {params.stop} 0 0" >> {output.popitable};fi'''
											
rule compile_pf_pi:
###compiles all ortholog-specific pi files into a single text file containing all orthologs for a given species
	input:
		expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-{pfgeneid}_{masked}_pi.txt", pfgeneid=list(i[0] for i in pf3d7_orthos_masked), allow_missing=True)		
	output:
		pfpi = config["output"]+"orthologs/overall/{project}_pf3d7_{mixed}_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.pfpi}; done"""

rule compile_poc_pi:
	input:
		expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-{pocgeneid}_{masked}_pi.txt", pocgeneid=list(i[0] for i in curtisigh01_orthos_masked), allow_missing=True)
	output:
		pocpi = config["output"]+"orthologs/overall/{project}_curtisigh01_{mixed}_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.pocpi}; done"""

rule compile_pow_pi:
	input:
		expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-{powgeneid}_{masked}_pi.txt", powgeneid=list(i[0] for i in wallikericr01_orthos_masked), allow_missing=True)
	output:
		powpi = config["output"]+"orthologs/overall/{project}_wallikericr01_{mixed}_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.powpi}; done"""



### Section _: Signatures of Selection

rule gunzip_vcfs:
###unzips monoclonal vcf files for use with selscan
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf"
	shell:
		"gunzip -c {input.vcf} > {output}"

rule remove_missing:
	###selscan requires no missing genotypes in order to impute selection, so we will filter out all sites that are missing any genotypes
	input: 
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf"
	output: 
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.recode.vcf"
	params:
		outfile = config["output"]+"sample_sets/{project}_{species}_{mixed}_allmaf_monoclonal_nomissing"
	conda: 
		"envs/filter.yaml"
	shell: 
		"vcftools --gzvcf {input} --out {params.outfile} --max-missing 1 --recode --recode-INFO-all"

rule gzip_nonmissing:
###rezips monoclonal sample set vcf file
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.recode.vcf"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"

rule index_nomissing_vcfs:
###generates index for non-missing monoclonal sample vcf files
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input}"""

rule subset_chr:
###subsets vcf files to only depict specific chromosomes for use in nSl calculations
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz",
		index = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz.tbi",
		bed = config["input_beds"]+"{species}_chr.bed"
	output:
		config["output"]+"chr_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing_{chromosome}.vcf.gz"
	conda:
		"envs/bcftools.yaml"
	shell:
		"bcftools view -r {wildcards.chromosome} {input.vcf} | gzip > {output}"
				
rule calc_n_sl:
	input:
		vcf = config["output"]+"chr_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing_{chromosome}.vcf.gz"
	output:
		config["output"]+"selection/{project}_{species}_{mixed}_nsl_{chromosome}.nsl.out"
	conda:
		"envs/selscan.yaml"
	params:
		outfile = config["output"]+"selection/{project}_{species}_{mixed}_nsl_{chromosome}"
	shell:
		"selscan --nsl --vcf {input.vcf} --out {params.outfile}"

rule compile_poc_n_sl:
###compiles the chromosome, position, and n_Sl calculated at all variants into a single text file 
	input:
		poc_nsl = expand(config["output"]+"selection/{project}_curtisigh01_{mixed}_nsl_{chromosome}.nsl.out", chromosome = curtisigh01_chr.keys(), allow_missing=True),
	output:
		poc_total = config["output"]+"selection/{project}_curtisigh01_{mixed}_nsl_total.txt"
	shell:
#		"""for i in {input.poc_nsl}; do cat $i | while read ID POS AF L1 L2 NSL; do echo "${{i%.nsl.out}} $POS $NSL" >> {output.poc_total}; done; done"""
		"""for i in {input.poc_nsl}; 
		do export CHROM=$(echo "$i" | cut -d "_" -f 5-6);
		cat $i | while read ID POS AF L1 L2 NSL; do echo "$CHROM $POS $NSL" >> {output.poc_total}; done; done;"""

rule compile_pow_n_sl:
###compiles the chromosome, position, and n_Sl calculated at all variants into a single text file 
	input:
		pow_nsl = expand(config["output"]+"selection/{project}_wallikericr01_{mixed}_nsl_{chromosome}.nsl.out", chromosome = wallikericr01_chr.keys(), allow_missing=True),
	output:
		pow_total = config["output"]+"selection/{project}_wallikericr01_{mixed}_nsl_total.txt"
	shell:
		"""for i in {input.pow_nsl};
		do export CHROM=$(echo "$i" | cut -d "_" -f 5 | cut -d "." -f 1);
		cat $i | while read ID POS AF L1 L2 NSL; do echo "$CHROM $POS $NSL" >> {output.pow_total}; done; done;"""

rule compile_pf_n_sl:
###compiles the chromosome, position, and n_Sl calculated at all variants into a single text file 
	input:
		pf_nsl = expand(config["output"]+"selection/{project}_pf3d7_{mixed}_nsl_{chromosome}.nsl.out", chromosome = pf3d7_chr.keys(), allow_missing=True),
	output:
		pf_total = config["output"]+"selection/{project}_pf3d7_{mixed}_nsl_total.txt"
	shell:
		"""for i in {input.pf_nsl};
		do export CHROM=$(echo "$i" | cut -d "_" -f 5-6);
		cat $i | while read ID POS AF L1 L2 NSL; do echo "$CHROM $POS $NSL" >> {output.pf_total}; done; done;"""



rule tajiima_d:
	#currently using monoclonal samples only and with only non-missing sites
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz"
	output:
		config["output"]+"selection/{project}_{species}_{mixed}.Tajima.D"
	params:
		outfile = config["output"]+"selection/{project}_{species}_{mixed}",
		window = config["tajima_window"]
	conda:
		"envs/filter.yaml"
	shell:
		"vcftools --gzvcf {input} --out {params.outfile} --TajimaD {params.window}"


rule select_tajima_d:
###collects tajima's D values within certain windows and compiles into two files
###one file shows the tajima's D within protein coding genes, the other does the same for exons only
### gene and exon bed files can include extrachromosomal contigs. Tajima's D values will onyl be extracted for loci with SNPs in the final analysis vcf
	input:
		stats = config["output"]+"selection/{project}_{species}_{mixed}.Tajima.D",
		genes = config["output"]+ "beds/{species}_genes_sorted.bed",
		exons = config["output"]+ "beds/{species}_exons_sorted.bed"	
	output:
		genes = config["output"]+"selection/{project}_{species}_{mixed}_tajimad_genes.txt",
		exons = config["output"]+"selection/{project}_{species}_{mixed}_tajimad_exons.txt"
	params:
		window = config["tajima_window"]
	script:
		"scripts/gene_exon_tajima.py"
		
		
