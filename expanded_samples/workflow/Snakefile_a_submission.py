### Snakemake Analysis pipeline for processing and statistical analyses of variants called across Poc and Pow genomes

#Setting up conda
#navigate to appropriate directory for data analysis
#Load anaconda through installation or local version
#module load anaconda/2021.11
#set up conda environment
#conda create -c conda-forge -c bioconda -n snakemake snakemake
#conda activate snakemake
#conda config --set channel_priority strict
###then run the snakefile from the snakemake/directory (Snakefile is the in the workflow directory)
#snakemake --use-conda --cores 1 --jobs 1 -s workflow/Snakefile

#needed input files:
#	vcf of variants called for sample(s) after aligning to a specific reference genome, formatted as "input/vcfs/{project}_{species}.vcf.gz"
#   for vcfs pulled from literature (Pf3D7 in this example), must limit to the specific samples and only sites that are variant among those samples (see "vcftools --non-ref-ac-any 1")
#   Pf3D7 vcf prepared from full Pf3K release 6 vcf using the script pf3k_rel-6_sample_subsetter.sh
#	the reference genome to which sample reads were aligned, formatted as "input/genomes/{species}.fasta"
#   index files for each reference genome, generated by "bwa-mem2 index", "samtools faidx", and "gatk CreateSequenceDictionary -R"
#	bed file of chromosome intervals for P. o. curtisi and P. o. wallikeri genomes to include (which excludes extrachromosomal contigs), formatted as "input/beds/{species}_chr.bed", also excludes Poc chr10 because hybrid capture did not contain baits to amplify most of this chromosome
#	bed file of specific expanded gene family intervals for the P. o. curtisi and P. o. wallikeri genomes, formatted as "input/beds/{species}_genemask.bed
#	bed file for P. falciparum core genome to be included, formatted as "input/beds/{species}_core.bed"
#	Determined hard quality filtering thresholds for ovale variants, given lack of literature on genomics of this organism
#	Use soft quality filter or preapplied quality filter for falciparum variants. Samples from Pf6k use the VQSLOD score filter
#	List of poc-pow one-to-one orthologs in the poc and pow genomes, in the same order, after subsetting and filtering using Snakefile_ortholog_prep.py
#	List of pf-poc-pow one-to-one-to-one orthologs in the poc, pow, and pf genomes, in the same order, after subsetting and filtering using Snakefile_ortholog_prep.py

configfile: "config/config.yaml"

###create list of sample names
samplenames = []
for i in open(config["input_lists"]+"sample_names.txt").readlines():
        samplenames.append(i.rstrip("\n"))

###for ortholog filtering later, we must create sets of pf, pow, and poc orthologs. Switched to list format
def parse_bed_file(bed_file):
	gene_list=[]
	for line in open(bed_file):
		name = line.split("\t")[0]
		chrom = line.split("\t")[1]
		start = line.split("\t")[2]
		stop = line.split("\t")[3]
		window = int(stop) - int(start) + 1
		gene_list.append([name, chrom, start, stop, window])
	return gene_list

pf3d7_triple_orthos_masked = parse_bed_file(config["output"]+ config["pf_triple_orthos_masked"])
curtisigh01_triple_orthos_masked = parse_bed_file(config["output"]+config["poc_triple_orthos_masked"])
wallikericr01_triple_orthos_masked = parse_bed_file(config["output"]+config["pow_triple_orthos_masked"])

curtisigh01_ovale_orthos_masked = parse_bed_file(config["output"]+config["poc_ovale_orthos_masked"])
wallikericr01_ovale_orthos_masked = parse_bed_file(config["output"]+config["pow_ovale_orthos_masked"])


###for nsl calculation, we also need a list of chromosome names for each species
###nSL is calculated for haplotypes on each chromosome, so we must subset vcfs by chromosome. This function derives a list of chromosome names from the chr.bed files
def	chrom_names(bed_file):
	chr_dict={}
	count = 0
	for line in open(bed_file):
		if count > 0:
			chrom = line.split("\t")[0]
			start = line.split("\t")[1]
			chr_dict[chrom]= [start]
		count += 1
	return chr_dict

pf3d7_chr = chrom_names(config["input_beds"]+ "pf3d7_chr.bed")
curtisigh01_chr = chrom_names(config["input_beds"]+ "curtisigh01_chr.bed")
wallikericr01_chr = chrom_names(config["input_beds"]+ "wallikericr01_chr.bed")


###Determines the final files to be output by the snakemake pipeline
rule all:
#Previous rules should use wildcards for the project and species, but this final rule should employ the actual names and terms for the final file to be created
	input:
		###vcf filtering and processing to create final analysis sets (including 5% MAF filter)
		files = expand(config["output"]+"analysis_vcfs/ov1_{species}_speciescall.vcf.gz", species = ["wallikericr01", "curtisigh01"]),
		pf_files = config["output"]+"analysis_vcfs/ov1_pf3d7_only.vcf.gz",
		tables = expand(config["output"]+"analysis_tables/ov1_{species}_speciescall.table", species = ["wallikericr01", "curtisigh01"]),
		pf_table = config["output"]+"analysis_tables/ov1_pf3d7_only.table",
		###Overal genome-wide statistics
		overall_snpden = expand(config["output"]+"statistics/overall_snp_density/ov1_{species}_speciescall.snpden", species =  ["wallikericr01", "curtisigh01"]),
		pf_snpden = config["output"]+"statistics/overall_snp_density/ov1_pf3d7_only.snpden",
		ovale_region_snp_density = expand(config["output"]+"statistics/region_snp_density/ov1_{species}_speciescall_snp_density.txt", species =  ["wallikericr01", "curtisigh01"]),
		pf_region_snp_density = config["output"]+"statistics/region_snp_density/ov1_pf3d7_only_snp_density.txt",
		###Annotation of vcfs
		annotated = expand(config["output"]+"annotated_vcfs/ov1_{species}_speciescall.vcf.gz", species = ["curtisigh01", "wallikericr01"]),
		ortholog_annotation = expand(config["output"]+"annotated_ortholog_vcfs/ov1_{species}_speciescall_ovaleorthologs.vcf.gz",species = ["curtisigh01", "wallikericr01"]),
		annotated_pf = config["output"]+"annotated_vcfs/ov1_pf3d7_only.vcf.gz",
		###Calculation of complexity of infection
		coi = expand(config["output"]+"statistics/coi/{species}_{mixed}/ov1_{species}_{mixed}_coi.txt", species = ["wallikericr01", "curtisigh01"], mixed = ["speciescall"]),
		coi_pf = config["output"]+"statistics/coi/pf3d7_only/ov1_pf3d7_only_coi.txt",
		###Generate vcfs of monoclonal samples only
		#mono_pf_file = config["output"]+"sample_sets/ov1_pf3d7_only_monoclonal_allmaf.vcf.gz",
		#mono_ovale_allmaf = expand(config["output"]+"sample_sets/ov1_{species}_{mixed}_monoclonal_allmaf.vcf.gz", species = ["wallikericr01", "curtisigh01"], mixed = ["speciescall"]),
		####Generate vcfs of each polyclonal samples alone
		wsaf= expand(config["output"]+"polyclonal_vcfs/ov1_{species}_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50-under50.vcf.gz", species = ["curtisigh01","wallikericr01"]),
		polyclonal_tables_curtisi = config["output"]+"sample_tables/ov1_curtisigh01_speciescall_366152-S10.table",
		polyclonal_tables_wallikeri = config["output"]+"sample_tables/ov1_wallikericr01_speciescall_113135-S5.table",
		polyclonal_tables_wallikeri2 = config["output"]+"sample_tables/ov1_wallikericr01_speciescall_353176-S8.table",
		###Generate plink files for PCA
		po_plink = expand(config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_{species}_speciescall/ov1_{species}_speciescall.eigenvec", species = ["wallikericr01", "curtisigh01"]),
		pf_plink = config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_pf3d7_only/ov1_pf3d7_only.eigenvec",
		###Admixture: Must alter this filepath to specify number of clusters to test for each species. If {cluster} goes up to 25, then works for up to 25 samples per species
		po_clusters = expand(config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_{species}_speciescall/run1/ov1_{species}_speciescall.clusters{cluster}.log", species = ["wallikericr01", "curtisigh01"], cluster = ["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25"]),
		pf_clusters = expand(config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/ov1_pf3d7_only/run1/ov1_pf3d7_only.clusters{cluster}.log", cluster = ["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18"]),
		###Calculation of piu among orthologs
		pfpi = expand(config["output"]+"orthologs/overall/ov1_pf3d7_only_triple_ortholog_{masked}_pi.txt", masked = ["masked"]),
		pocpi = expand(config["output"]+"orthologs/overall/ov1_curtisigh01_speciescall_triple_ortholog_{masked}_pi.txt", masked = ["masked"]),
		powpi = expand(config["output"]+"orthologs/overall/ov1_wallikericr01_speciescall_triple_ortholog_{masked}_pi.txt", masked = ["masked"]),
		ovale_pocpi = expand(config["output"]+"orthologs/overall/ov1_curtisigh01_speciescall_ovale_ortholog_{masked}_pi.txt", masked = ["masked"]),
		ovale_powpi = expand(config["output"]+"orthologs/overall/ov1_wallikericr01_speciescall_ovale_ortholog_{masked}_pi.txt", masked = ["masked"]),
		###Calculation of nSL
		nsl_po_species = expand(config["output"]+"selection/ov1_{species}_speciescall_nsl_total.txt", species = ["wallikericr01", "curtisigh01"]),
		nsl_pf_compile = config["output"]+"selection/ov1_pf3d7_only_nsl_total.txt",
		### Tajima's D calculations
		tajima_compiled = expand(config["output"]+"selection/ov1_{species}_speciescall_tajimad-"+config["tajima_window"]+"_{elements}.bed", species = ["wallikericr01", "curtisigh01"], elements = ["genes","exons"]),
		tajima_labeled = expand(config["output"]+"selection/ov1_{species}_speciescall_tajimad-"+config["tajima_window"]+"_genes_IDs.bed", species = ["wallikericr01", "curtisigh01"]),
		###Snakemake administrative###
		config = config["output"]+"pipeline/config.yaml",
		snakefile = config["output"]+"pipeline/Snakefile_analysis.py"

### Step 1: filter vcfs to core genome only
###
###

rule index_input_vcfs:
###Generare index files for vcfs generated in previous snakemake file
	input:
		config["output"]+"vcfs/{project}/{project}_{species}_{mixed}.vcf.gz"
	output:
		config["output"]+"vcfs/{project}/{project}_{species}_{mixed}.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input}"""

###For samples aligned to the PocGH01 or PowCR01 genomes, this limits the SNPs to chromosomal contigs only
rule mask_Po_chr:
	input:
	#This step requires that the ovale species names both end in "01", while the falciparum species name does not. This causes this step to only be run on ovale samples
		vcf = config["output"]+"vcfs/{project}/{project}_{ovale}01_{mixed}.vcf.gz",
		index = config["output"]+"vcfs/{project}/{project}_{ovale}01_{mixed}.vcf.gz.tbi",
		bed = config["input_beds"]+"{ovale}01_chr.bed"
	params:
		prefix = config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked"
	output:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"vcftools --gzvcf {input.vcf} --out {params.prefix} --bed {input.bed} --recode --recode-INFO-all"

###Recodes default output of mask_Po_chr as gzipped vcf files
rule recode_Po_chrmask:
	input:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.recode.vcf"
	output:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.vcf.gz"	
	shell:
		"bgzip -c {input} > {output}"

###For samples aligned to PocGH01 or PowCR01, maks SNPs in specific hypervariable gene families shown in Rutledge 2017
rule mask_Po_variablegenes:
	input:
		vcf = config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_chrmasked.vcf.gz",
		bed = config["input_beds"]+"{ovale}01_genemask.bed"
	params:
		prefix = config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_masked"
	output:
		config["output"]+"masked_vcfs/{project}_{ovale}01_{mixed}_masked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"""vcftools --gzvcf {input.vcf} --out {params.prefix} --exclude-bed {input.bed} --recode --recode-INFO-all"""


###For samples aligned to Pf3D7, this limits SNPs to the standard Pf3D7 core genome (masking telomeres and some specific gene families/regions)
rule mask_Pf_acessory_genome:
	input: 
		vcf = config["output"]+"vcfs/{project}/{project}_pf3d7_{mixed}.vcf.gz",
		index =  config["output"]+"vcfs/{project}/{project}_pf3d7_{mixed}.vcf.gz.tbi",
		bed = config["input_beds"]+"pf3d7_core.bed"
	params:
		prefix = config["output"]+"masked_vcfs/{project}_pf3d7_{mixed}_masked"
	output: config["output"]+"masked_vcfs/{project}_pf3d7_{mixed}_masked.recode.vcf"
		#this command outputs "masked_vcfs/{project}_pf3d7_only_masked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"""vcftools --gzvcf {input.vcf} --out {params.prefix} --bed {input.bed} --recode --recode-INFO-all"""


###final recoding after masking for all species
rule recode_masked:
	input:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.recode.vcf"
	output:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"

### produces index (.tbi) files for all masked vcfs
rule index_masked_vcfs:
	input:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz"
	output:
		config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input}"""



### Step 2: quality filter sites based on QC scores, minor allele frequency, and individual-level missingness
###
###


###limits to biallelic SNPs
rule biallelic:
	input:
		vcf = config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz",
		index = config["output"]+"masked_vcfs/{project}_{species}_{mixed}_masked.vcf.gz.tbi"
	output:
		config["output"]+"masked_biallelic_vcfs/{project}_{species}_{mixed}_masked_biallelic.vcf.gz"
	params:
		reference = config["input_genomes"]+"{species}.fasta"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants -R {params.reference}  -V {input.vcf} --select-type-to-include SNP --restrict-alleles-to BIALLELIC -O {output}"

### Generate table of SNPs with quality metrics
rule table_for_hardfilter:
	input:
		config["output"]+"masked_biallelic_vcfs/{project}_{species}_{mixed}_masked_biallelic.vcf.gz"
	output:
		config["output"]+"variant_tables/unqfiltered/{project}_{species}_{mixed}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input} -F CHROM -F POS -F QD -F FS -F SOR -F MQ -F MQRankSum -F ReadPosRankSum -O {output}"


###adds a filter-tag to all SNPs who fail to pass the following hard quality thresholds
rule po_qualfiltertagger:
	input:
		vcf = config["output"]+"masked_biallelic_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic.vcf.gz",
		reference = config["input_genomes"]+"{ovale}01.fasta",
		#table is not needed but should be generated and evaluated before hard-filtering
		table = config["output"]+"variant_tables/unqfiltered/{project}_{ovale}01_{mixed}.table"
	output:
		config["output"]+"filtered_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic_filtertag.vcf"
	params:
		filters = config["gatk_hardfiltersnps"],
		java_opts = config["general_java_opts"]
	conda:
		"envs/biallelic.yaml"
	threads: 2
	shell:
		"""gatk --java-options "{params.java_opts}" VariantFiltration {params.filters} -R {input.reference} -V {input.vcf} -O {output}"""
	

###gzip-compresses output vcf from rule po_qualfiltertagger
rule po_qual_gzip:
	input:
		config["output"]+"filtered_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic_filtertag.vcf"
	output:
		config["output"]+"filtered_vcfs/{project}_{ovale}01_{mixed}_masked_biallelic_filtertag.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"		

###adds a filter-tag to all SNPs who fail the VQSLOD quality score, a default metric provided by Pf6k
###This score and filter-tag are already included, so this rule just copies and indexes the same vcf file
rule pf_qualfiltertagger:
	input:
		vcf = config["output"]+"masked_biallelic_vcfs/{project}_pf3d7_{mixed}_masked_biallelic.vcf.gz",
		reference = config["input_genomes"]+"pf3d7.fasta"
	output:
		config["output"]+"filtered_vcfs/{project}_pf3d7_{mixed}_masked_biallelic_filtertag.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"""cp {input.vcf} {output}
		gatk IndexFeatureFile -I {output}"""

###produces index (.tbi) files for all qc-filter-tagged vcfs
rule index_tagged_vcfs:
		input:
			config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz"
		output:
			config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz.tbi"
		conda:
			"envs/masker.yaml"
		shell:
			"""bcftools index -t {input}"""
			
###Filter out all SNPs that fail QC (by hard filtering threshold for Po or the VQSLOD score for Pf) based on the presence of the filter tag
rule filter_by_tag:
	input: 
		vcf = config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz",
		index = config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_filtertag.vcf.gz.tbi",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants -R {input.reference} -V {input.vcf} -O {output} --exclude-filtered"

### generate table of SNPs that pass quality filters alongside their quality metrics
rule eval_hard_filter:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz"
	output:
		config["output"]+"variant_tables/qfiltered/{project}_{species}_{mixed}_qfiltered.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input} -F CHROM -F POS -F QD -F FS -F SOR -F MQ -F MQRankSum -F ReadPosRankSum -O {output}"


###remove sites with tandem repeats
###first three rules split individual chromosomes from reference genomes
rule split_poc_fasta_chr:
	input:
		ref = config["input_genomes"]+"curtisigh01.fasta"
	output:
		expand(config["output"] + "chr_fasta/curtisigh01-{chromosome}.fasta", chromosome = curtisigh01_chr.keys(), allow_missing = True),
	params:
		outfile = config["output"] + "chr_fasta/curtisigh01-"
	shell:
		"""csplit -s -z {input.ref} '/>/' '{{*}}'
		for i in xx* ; do \
		  n=$(sed 's/>// ; s/ .*// ; 1q' "$i") ; \
		  mv "$i" "{params.outfile}$n.fasta" ; \
		 done"""

rule split_pow_fasta_chr:
	input:
		ref = config["input_genomes"]+"wallikericr01.fasta"
	output:
		expand(config["output"] + "chr_fasta/wallikericr01-{chromosome}.fasta", chromosome = wallikericr01_chr.keys(), allow_missing = True),
	params:
		outfile = config["output"] + "chr_fasta/wallikericr01-"
	shell:
		"""csplit -s -z {input.ref} '/>/' '{{*}}'
		for i in xx* ; do \
		  n=$(sed 's/>// ; s/ .*// ; 1q' "$i") ; \
		  mv "$i" "{params.outfile}$n.fasta" ; \
		 done"""

rule split_pf_fasta_chr:
	input:
		ref = config["input_genomes"]+"pf3d7.fasta"
	output:
		expand(config["output"] + "chr_fasta/pf3d7-{chromosome}.fasta", chromosome = pf3d7_chr.keys(), allow_missing = True),
	params:
		outfile = config["output"] + "chr_fasta/pf3d7-"
	shell:
		"""csplit -s -z {input.ref} '/>/' '{{*}}'
		for i in xx* ; do \
		  n=$(sed 's/>// ; s/ .*// ; 1q' "$i") ; \
		  mv "$i" "{params.outfile}$n.fasta" ; \
		 done"""

###THIS SECTION IS NOT WORKING, REORIENTING UNDER ASSUMPTION THAT ERROR CODES ARE ERRONEOUS
#next rule identifies tandem repeats within each chromosome	
rule tandem_repeat_finder:
	input:
		ref = config["output"]+"chr_fasta/{species}-{chromosome}.fasta",
	output:
#		#config["output"]+"trfs/{species}_{chromosome}.fasta.2.7.7.80.10.50.500.dat"
		"{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat"
	params:
		outdir = config["output"]+"trfs/",
		outfile = "{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat",
		rel_ref = "../../"+config["output"]+"chr_fasta/{species}-{chromosome}.fasta",
	conda:
		"envs/trf.yaml"
	shell:
#		#removed -h to allow html output for troubleshooting
		"""set +e
		echo $PWD
		trf {input.ref} 2 7 7 80 10 50 500 -h
		echo $?
		exitcode=$?
		if [ $exitcode -eq 1 ]
			then
				exit 1
			else
				exit 0
		fi"""



rule move_tandem_repeat_files:
#moves output files from step that identifies tandem repeats
	input:
		"{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat"
	output:
		config["output"]+"trfs/{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat"
	shell:
		"mv {input} {output}"


#####ATTEMPTING TO TROUBLESHOOT NON-ZERO EXIT status in TRF
#rule tandem_repeat_finder:
#	input:
#		ref = config["output"]+"chr_fasta/{species}-{chromosome}.fasta",
#	output:
#		config["output"]+"trfs/{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat"
#	params:
#		intermed = "{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat"
#	conda:
#		"envs/trf.yaml"
#	run:
#		shell("trf {input.ref} 2 7 7 80 10 50 500 -h")
#		shell("mv {params.intermed} {output}")


rule noheader_tandem_repeat:
#removes header section from tandem repeat file, so just a list of tandem repeats
	input:
		config["output"]+"trfs/{species}-{chromosome}.fasta.2.7.7.80.10.50.500.dat"
	output:
		config["output"]+"trfs/{species}-{chromosome}_trf_noheader.dat"
	shell:
		"awk '$1 ~ /^[0-9]+$/ {{print $0}}' {input} > {output}"

rule create_tandem_repeat_bed:
#selects start and stop positive of tandem repeats in each chromsome, creating a by-chromosome repeat bed file
	input:
		config["output"]+"trfs/{species}-{chromosome}_trf_noheader.dat"
	output:
		config["output"]+"trfs/{species}-{chromosome}_trf.bed"
	shell:
		"""cat {input} | while read START STOP A B C D E F G H I J K L M; do echo "{wildcards.chromosome}\t$START\t$STOP" >> {output}; done"""

rule merge_poc_tandem_beds:
#concatenates tandem repeat bed files for each chromosome to make final bed file for poc
	input:
		expand(config["output"]+"trfs/curtisigh01-{chromosome}_trf.bed", chromosome = curtisigh01_chr.keys(), allow_missing = True),
	output:
		config["output"]+"beds/curtisigh01_trf.bed"
	shell:
		"cat {input} > {output}"

rule merge_pow_tandem_beds:
#concatenates tandem repeat bed files for each chromosome to make final bed file for pow
	input:
		expand(config["output"]+"trfs/wallikericr01-{chromosome}_trf.bed", chromosome = wallikericr01_chr.keys(), allow_missing = True),
	output:
		config["output"]+"beds/wallikericr01_trf.bed"
	shell:
		"cat {input} > {output}"

rule merge_pf_tandem_beds:
#concatenates tandem repeat bed files for each chromosome to make final bed file for pof
	input:
		expand(config["output"]+"trfs/pf3d7-{chromosome}_trf.bed", chromosome = pf3d7_chr.keys(), allow_missing = True),
	output:
		config["output"]+"beds/pf3d7_trf.bed"
	shell:
		"cat {input} > {output}"

rule mask_trfs:
# masks tandem repeat intervals in the vcfs
	input:
		vcf = config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered.vcf.gz",
		bed = config["output"]+"beds/{species}_trf.bed"
	params:
		prefix = config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"""vcftools --gzvcf {input.vcf} --out {params.prefix} --exclude-bed {input.bed} --recode --recode-INFO-all"""

#recodes resulting vcf
rule recode_trf_masked:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked.recode.vcf"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"	

###remove sites with a MAF lower than a threshold of 5%
rule filter_maf:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked.vcf.gz"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmaffiltered.vcf.gz"
	conda:
		"envs/filter.yaml"
	shell:
		"vcftools --gzvcf {input} --out {output} --recode --maf 0.05 --stdout | bgzip > {output}"

###only keep sites which are present (nonmissing) in XX% of samples
rule filter_missing:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmaffiltered.vcf.gz"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmafmissfiltered.vcf.gz"
	conda:
		"envs/filter.yaml"
	shell:
#This command limits the vcf to sites which were present in at least 80% of individuals
		"vcftools --gzvcf {input} --out {output} --max-missing 0.8 --recode --stdout | bgzip > {output}"

###Creates final cleaned vcf for downstream analysis
rule final_set:
#moves annotated processed vcfs to a new directory for further analysis
#vcfs in the analysis_vcfs directory have been limited to the core genome (in ovale, this means chromosomal contigs with a specific list 
# of hypervariable genes manually masked based on Rutledge 2017; limited to biallelic snps; and quality filtered by hard thresholds (in ovale) or the 
# default VQSLOD filter applied in the Pf6k dataset (for falciparum), minor allele frequency of 5%, and presence in >80% of samples 
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmafmissfiltered.vcf.gz"
	output:
		config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	shell:
		"cp {input} {output}"

#generates index files for final vcfs
rule index_final_vcfs:
	input:
		vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
	output:
		config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

rule final_table:
#generates table of SNPs in final callset
	input:
		vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
	output:
		config["output"]+"analysis_tables/{project}_{species}_{mixed}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input.vcf} -F CHROM -F POS -F QD -F FS -F SOR -F MQ -F MQRankSum -F ReadPosRankSum -O {output}"

### Create final vcfs without minor allele frequency filter
###only keep sites which are present (nonmissing) in 80% of samples
rule filter_missing_allmaf:
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked.vcf.gz"
	output:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmissfiltered.vcf.gz"
	conda:
		"envs/filter.yaml"
	shell:
#This command limits the vcf to sites which were present in at least 80% of individuals
		"vcftools --gzvcf {input} --out {output} --max-missing 0.8 --recode --stdout | bgzip > {output}"

###Creates final cleaned vcf for downstream analysis
rule final_set_allmaf:
#moves annotated processed vcfs to a new directory for further analysis
#vcfs in the analysis_vcfs directory have been limited to the core genome (in ovale, this means chromosomal contigs with a specific list 
# of hypervariable genes manually masked based on Rutledge 2017; limited to biallelic snps; and quality filtered by hard thresholds (in ovale) or the 
# default VQSLOD filter applied in the Pf6k dataset (for falciparum), and presence in >80% of samples 
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qmissfiltered.vcf.gz"
	output:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	shell:
		"cp {input} {output}"


rule index_final_vcfs_allmaf:
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

### Step 3
### Determine SNP density across the genome and within specific functional portions of the genome

rule overall_snp_density_vcftools:
#calculates genome-wide SNP density per 1Kb
	input:
		config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
		#config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	output:
		config["output"]+"statistics/overall_snp_density/{project}_{species}_{mixed}.snpden"
	params:
		outfile = config["output"]+"statistics/overall_snp_density/{project}_{species}_{mixed}"
	conda:
		"envs/filter.yaml"
	shell:
		"""vcftools --gzvcf {input} --SNPdensity 1000 --out {params.outfile}"""

###To calculate SNP Density at various regions of the genome, we must import bed files of genes, exons, CDS from the gff file
### a bed file of chromosome sizes can be produced from the chromosome bed file using : cat curtisigh01_chr.bed | while read CHROM START STOP; do echo -e "$CHROM\t$STOP" >> curtisigh01_chrsize.bed; done

rule chromosome_size_bed:
#creates text file with length of each chromosome
	input:
		config["input_beds"]+ "{species}.bed"
	output:
		config["output"]+ "beds/{species}_chrsize.txt"
	shell:
		'''cat {input} | while read CHROM START STOP; do echo -e "$CHROM\t$STOP" | sed 's/\r$//' >> {output}; done'''


rule sort_gffs:
#sorts gff files into chromosomal order
	input:
		config["input"]+"gffs/{species}.gff"
	output:
		config["output"]+"gffs/{species}_sorted.gff"
	shell:
		'''cat {input} | awk '$1 ~ /^#/ {{print $0;next}} {{print $0 | "sort -k1,1 -k4,4n -k5,5n"}}' > {output}'''

rule intergenic_bed:
# generates bed file of intergenic regions of the genome
	input:
		gff_sorted = config["output"]+"gffs/{species}_sorted.gff",
		chrsize_bed = config["output"]+ "beds/{species}_chrsize.txt"
	output:
		config["output"]+"beds/{species}_intergenic_sorted.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		'''bedtools complement -i {input.gff_sorted} -g {input.chrsize_bed} > {output}'''

rule exon_bed:
# generates bed file of exon regions of the genome
	input:
		config["output"]+"gffs/{species}_sorted.gff"
	output:
		config["output"]+ "beds/{species}_exons_sorted.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 == "exon") print $1, $4-1, $5}}' {input} > {output}'''

rule gene_bed:
# generates bed file of gene regions of the genome
	input:
		config["output"]+"gffs/{species}_sorted.gff"
	output:
		config["output"]+ "beds/{species}_genes_sorted.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 ~ "gene") print $1, $4-1, $5}}' {input} > {output}'''

rule geneID_poc_bed:
# generates bed file of gene regions of the genome alongside gene IDs
	input:
		config["output"]+"gffs/curtisigh01_sorted.gff"
	output:
		config["output"]+ "beds/curtisigh01_genes_sorted_ids.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 ~ "gene") print $1, $4-1, $5, substr($9,4,16)}}' {input} > {output}'''

rule geneID_pow_bed:
# generates bed file of gene regions of the genome alongside gene IDs
	input:
		config["output"]+"gffs/wallikericr01_sorted.gff"
	output:
		config["output"]+ "beds/wallikericr01_genes_sorted_ids.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 ~ "gene") print $1, $4-1, $5, substr($9,4,17)}}' {input} > {output}'''

rule cds_bed:
# generates bed file of CDS regions of the genome alongside gene IDs
	input:
		config["output"]+"gffs/{species}_sorted.gff"
	output:
		config["output"]+ "beds/{species}_cds_sorted.bed"
	shell:
		'''awk 'OFS="\t", $1 !~ /^#/ {{if ($3 == "CDS") print $1, $4-1, $5}}' {input} > {output}'''

rule intron_bed:
# generates bed file of intron regions of the genome alongside gene IDs
	input:
		exons = config["output"]+ "beds/{species}_exons_sorted.bed",
		intergenic = config["output"]+"beds/{species}_intergenic_sorted.bed",
		chrsize_bed = config["output"]+ "beds/{species}_chrsize.txt"
	output:
		config["output"]+"beds/{species}_introns_sorted.bed"
	shell:
		'''bedtools complement -i <(cat {input.exons} {input.intergenic} | sort -k1,1 -k2,2n) -g {input.chrsize_bed} > {output}'''

###for introns, exons, cds, genes, and intergenic region beds, need to remove extrachromosomal contigs and have sorted
rule remove_extrachromosomal:
	input:
		region_bed = config["output"]+ "beds/{species}_{region}_sorted.bed",
		chr_bed = config["input_beds"]+"{species}_chr.bed",
	output:
		config["output"]+ "beds/{species}_{region}_sorted_chrselected.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools multiinter -i {input.region_bed} {input.chr_bed} | awk '$4==2' > {output}"

rule genemask_ovale_region_beds:
#for specific  genome region bed files, removes the intervals masked by the hypervariable gene mask
	input:
		region_bed = config["output"]+ "beds/{ovale}01_{region}_sorted_chrselected.bed",
		genemask_bed = config["input_beds"]+"{ovale}01_genemask.bed",
	output:
		config["output"]+ "beds/{ovale}01_{region}_sorted_chrselected_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.genemask_bed} > {output}"


rule trfmask_ovale_region_beds:
#removes TRF regions from the corresponding bed file
	input:
		region_bed = config["output"]+ "beds/{ovale}01_{region}_sorted_chrselected_genemasked.bed",
		bed = config["output"]+"beds/{ovale}01_trf.bed"
	output:
		config["output"]+ "beds/{ovale}01_{region}_sorted_chrselected_masked.bed",
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.bed} > {output}"

rule corelimit_pf3d7_region_beds:
#removes non-core intervals from Pf3D7 bed files
	input:
		region_bed = config["output"]+ "beds/pf3d7_{region}_sorted_chrselected.bed",
		core_bed = config["input_beds"]+"pf3d7_core.bed",
	output:
		config["output"]+ "beds/pf3d7_{region}_sorted_chrselected_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools multiinter -i {input.region_bed} {input.core_bed} | awk '$4==2' > {output}"

rule trfmask_pf3d7_region_beds:
# removes tandem repeats from the PF3D7 bed files
	input:
		region_bed = config["output"]+ "beds/pf3d7_{region}_sorted_chrselected_genemasked.bed",
		bed = config["output"]+"beds/pf3d7_trf.bed"
	output:
		config["output"]+ "beds/pf3d7_{region}_sorted_chrselected_masked.bed",
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.bed} > {output}"

rule genemask_ovale_chr_bed:
### Removes expanded gene family intervals from P. ovale bed files
	input:
		region_bed = config["input_beds"]+ "{ovale}01_chr.bed",
		genemask_bed = config["input_beds"]+"{ovale}01_genemask.bed",
	output:
		config["output"]+ "beds/{ovale}01_chr_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.genemask_bed} > {output}"

rule trfmask_ovale_chr_bed:
### Removes tandem repeat  intervals from P. ovale bed files
	input:
		region_bed = config["output"]+ "beds/{ovale}01_chr_genemasked.bed",
		bed = config["output"]+"beds/{ovale}01_trf.bed",
	output:
		config["output"]+ "beds/{ovale}01_chr_masked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.bed} > {output}"

rule corelimit_pf3d7_chr_bed:
# limits Pf3d7 whole chromosome bed file to core genome intervals
	input:
		region_bed = config["input_beds"]+ "pf3d7_chr.bed",
		core_bed = config["input_beds"]+"pf3d7_core.bed",
	output:
		config["output"]+ "beds/pf3d7_chr_genemasked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools multiinter -i {input.region_bed} {input.core_bed} | awk '$4==2' > {output}"

rule trfmask_pf3d7_chr_bed:
# removes tandem repeats from Pf3D7 chromosome bed file
	input:
		region_bed = config["output"]+ "beds/pf3d7_chr_genemasked.bed",
		bed = config["output"]+"beds/pf3d7_trf.bed",
	output:
		config["output"]+ "beds/pf3d7_chr_masked.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools subtract -a {input.region_bed} -b {input.bed} > {output}"

rule region_snp_table:
# generates table of SNPs within each genomic region
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		#vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		#index = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		bed = config["output"]+ "beds/{species}_{region}_sorted_chrselected_masked.bed"
	output:
		config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_{region}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input.vcf} -L {input.bed} -F CHROM -F POS -O {output}"

rule genome_snp_table:
# generates table of SNPs across the whole genome
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		bed = config["output"]+ "beds/{species}_chr_masked.bed"
	output:
		config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_genome.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input.vcf} -L {input.bed} -F CHROM -F POS -O {output}"

rule merge_overlap:
#merge bed file windows so that overlapping genome elements do not falsely inflate region length for SNP density calculations (ie two overlapping exons from different genes shouldn't be counted as separate regions of the genome)
	input:
		config["output"]+ "beds/{species}_{region}_sorted_chrselected_masked.bed"
	output:
		config["output"]+ "beds/{species}_{region}_sorted_chrselected_masked_merged.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		"bedtools merge -i {input} > {output}"				

###for each type of region, make a script that counts SNPs within the SNPs within the regions and divides by the total legnth of intervals in each bed file
# first, do a gatk varianttotable step using -L with the corresponding bed files
# then, take in the table and the bed file and do a simple calculation of numbers of rows in table divided by total length of intervals in bed file
rule compile_snp_density:
	input:
		genome_bed = config["output"]+ "beds/{species}_chr_masked.bed",
		intergenic_bed = config["output"]+"beds/{species}_intergenic_sorted_chrselected_masked_merged.bed",
		gene_bed = config["output"]+ "beds/{species}_genes_sorted_chrselected_masked_merged.bed",
		intron_bed =  config["output"]+"beds/{species}_introns_sorted_chrselected_masked_merged.bed",
		exon_bed = config["output"]+ "beds/{species}_exons_sorted_chrselected_masked_merged.bed",
		cds_bed = config["output"]+ "beds/{species}_cds_sorted_chrselected_masked_merged.bed",
		genome_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_genome.table",
		intergenic_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_intergenic.table",
		gene_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_genes.table",
		intron_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_introns.table",
		exon_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_exons.table",
		cds_table = config["output"]+"allmaf_region_tables/{project}_{species}_{mixed}_cds.table",
		#genome_table = config["output"]+"region_tables/{project}_{species}_{mixed}_genome.table",
		#intergenic_table = config["output"]+"region_tables/{project}_{species}_{mixed}_intergenic.table",
		#gene_table = config["output"]+"region_tables/{project}_{species}_{mixed}_genes.table",
		#intron_table = config["output"]+"region_tables/{project}_{species}_{mixed}_introns.table",
		#exon_table = config["output"]+"region_tables/{project}_{species}_{mixed}_exons.table",
		#cds_table = config["output"]+"region_tables/{project}_{species}_{mixed}_cds.table",
	output:
		config["output"]+"statistics/region_snp_density/{project}_{species}_{mixed}_snp_density.txt"
	script:
		"scripts/region_snp_density_compiler.py"



### Step 4: generate outfiles for calculation of Complexity of Infection using THEREALMcCOIL


#### Trying with and without MAF filter
rule submit_mccoilr_script:
	input:
		#config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}_genemasked.vcf.gz"
		config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz"
	output:
		config["output"]+"statistics/coi/{species}_{mixed}/{project}_{species}_{mixed}_coi.txt"
	conda:
		"envs/r.yaml"
	params:
		maxcoi = config["realmccoilr"]["maxcoi"],
		threshold_ind = config["realmccoilr"]["threshold_ind"],
		threshold_site = config["realmccoilr"]["threshold_site"],
		totalrun = config["realmccoilr"]["totalrun"],
		burnin = config["realmccoilr"]["burnin"],
	resources:
		mem_mb = 300000
	script:	
		"scripts/Complexity_of_infection.R"


###Based on summary from REALMcCOIL, generate polyclonal input sample lists 
rule subset_monoclonal:
###excludes polyclonal samples, as determined using RealMcCOIl
### of note, vcf files will still contain information for sites which were all reference for monoclonal samples (shouldn't impact pi or selection calculations)
### but inclusion of these sites would impact investigation of specific SNP sites (some sites are not SNPs in this dataset)
	input:
		vcf = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		samplelist = config["input_lists"] + "{project}_polyclonalsamples.args",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --exclude-sample-name {input.samplelist} -R {input.reference} -V {input.vcf} -O {output}"


###Prepare allele-frequency filtered vcfs for examination of polyclonal samples

rule conservative_MAF_filter:
	##filter sites with less than 10% MAF
	input:
		config["output"]+"filtered_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_trfmasked.vcf.gz"
	output:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10.vcf.gz"
	conda:
		"envs/filter.yaml"
	#### ADD a param to allow adjusting MAF, gauge whether MAF is suffieicnt for robustenss to sequencing error
	shell:
#the following command removes any sites with a minor allele frequency less than 10%
		"vcftools --gzvcf {input} --out {output} --recode --maf 0.1 --stdout | bgzip > {output}"

###only keep sites which are present (nonmissing) in 80% of samples
rule polyclonal_missing_filter:
	input:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10.vcf.gz"
	output:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80.vcf.gz"
	conda:
		"envs/filter.yaml"
	shell:
#This command limits the vcf to sites which were present in at least 80% of individuals
		"vcftools --gzvcf {input} --out {output} --max-missing 0.8 --recode --stdout | bgzip > {output}"

rule add_wsaf:
	###Add within-sample allele frequency format field to each sample-site combination for further filtering
	input:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80.vcf.gz"
	output:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_wsaf.vcf.gz"
	conda:
		"envs/bcftools.yaml"
	shell:
		"bcftools +fill-tags {input} -Oz -o {output} -- -t  FORMAT/VAF"


###The next two rules will filter by alternate variant alelle frequency across all samples
### The purpose is to limit only to sites in which the minor allele was the major allele for at least one sample in the population
### REF and ALT alleles are arbitrary in this reference, and variant allele frequency is calculated at each site for all samples as the frequency of the alternate allele
### To limit bby minor allele, we will do two steps
###The first requires at least one VAF to be greater than 0.5. If the ALT allele is the minor allele, then this meets our desired outcome. If the ALT allele is the major allele, it will certainly be over 50% of reads in at least one sample
### The second rule requires at least one VAF to be less than 0.5. In the remaining sites where the ALT allele is the major allele, this will constitute a sample in which the minor allele (REF) was more than half the reads
rule filter_high_vaf:
	###limit only to sites in which at least one sample had the ALT allele as over 50% of reads
	## If ALT allele is major allele, this will affect nothing. If ALT allele is minor allele, this will require it was major in at least one sample
	input:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_wsaf.vcf.gz"
	output:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz"
	conda:
		"envs/bcftools.yaml"
	shell:
		"bcftools filter -i 'FORMAT/VAF[*]>0.5' {input} -Oz -o {output}"

rule filter_low_vaf:
	### limit only to sites in which at least one sample had the ALT allele as less than half of the reads
	### If the ALT allele is the major, then this would require that the minor allele was predominant in at least one sample
	input:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz"
	output:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_vaf-over50-under50.vcf.gz"
	conda:
		"envs/bcftools.yaml"
	shell:
		"bcftools filter -i 'FORMAT/VAF[*]<0.5' {input} -Oz -o {output}"


rule index_vaf_filter:
	input:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz"
	output:
		config["output"]+"polyclonal_vcfs/{project}_{species}_{mixed}_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz.tbi"
	conda:
		"envs/bcftools.yaml"
	shell:
		"""bcftools index -t {input}"""

### The next two rules are hand-written to generate the specific vcf files of one polyclonal sample each for investigation
### Will need to be manually curated for each analysis
rule generate_curtisi_S10:
	input:
		vcf = config["output"]+"polyclonal_vcfs/ov1_curtisigh01_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz",
		index = config["output"]+"polyclonal_vcfs/ov1_curtisigh01_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz.tbi",
		samplelist = config["input_lists"] + "ov1_366152_S10.args",
		reference = config["input_genomes"]+"curtisigh01.fasta"
	output:
		config["output"]+"sample_sets/ov1_curtisigh01_speciescall_366152-S10.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --sample-name {input.samplelist} --exclude-non-variants -R {input.reference} -V {input.vcf} -O {output}"

rule generate_wallikeri_S5:
	input:
		vcf = config["output"]+"polyclonal_vcfs/ov1_wallikericr01_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz",
		index = config["output"]+"polyclonal_vcfs/ov1_wallikericr01_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz.tbi",
		samplelist = config["input_lists"] + "ov1_113135_S5.args",
		reference = config["input_genomes"]+"wallikericr01.fasta"
	output:
	#to avoid ambiguity, make sure that sample ID + number does not contain a "_" character (replaced with dash)
		config["output"]+"sample_sets/ov1_wallikericr01_speciescall_113135-S5.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --sample-name {input.samplelist} --exclude-non-variants -R {input.reference} -V {input.vcf} -O {output}"

rule generate_wallikeri_S8:
	input:
		vcf = config["output"]+"polyclonal_vcfs/ov1_wallikericr01_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz",
		index = config["output"]+"polyclonal_vcfs/ov1_wallikericr01_speciescall_masked_biallelic_qfiltered_maf10_miss80_vaf-over50.vcf.gz.tbi",
		samplelist = config["input_lists"] + "ov1_353176_S8.args",
		reference = config["input_genomes"]+"wallikericr01.fasta"
	output:
	#to avoid ambiguity, make sure that sample ID + number does not contain a "_" character (replaced with dash)
		config["output"]+"sample_sets/ov1_wallikericr01_speciescall_353176-S8.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --sample-name {input.samplelist} --exclude-non-variants -R {input.reference} -V {input.vcf} -O {output}"


#####filter polyclonal vcfs to sites with within-sample variant allele frequency between 5% and 95%
rule wsaf_polyclonal_filter:
	input:
		config["output"]+"sample_sets/ov1_{species}_speciescall_{sample}.vcf.gz"
	output:
		config["output"]+"sample_sets/ov1_{species}_speciescall_{sample}_affilter.vcf.gz"
	conda:
		"envs/bcftools.yaml"
	shell:
		"bcftools filter -i 'FORMAT/VAF[*]>0.05 && FORMAT/VAF[*]<0.95' {input} -Oz -o {output}"

rule index_wsaf_polyclonal_filter:
	input:
		config["output"]+"sample_sets/ov1_{species}_speciescall_{sample}_affilter.vcf.gz"
	output:
		config["output"]+"sample_sets/ov1_{species}_speciescall_{sample}_affilter.vcf.gz.tbi"
	conda:
		"envs/bcftools.yaml"
	shell:
		"""bcftools index -t {input}"""
		

### to examine SNP and het call distributions, generate a table from each single-sample polyclonal vcf files
rule polyclonal_table:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_speciescall_{sample}_affilter.vcf.gz",
		index = config["output"]+"sample_sets/ov1_{species}_speciescall_{sample}_affilter.vcf.gz.tbi"
	output:
		config["output"]+"sample_tables/{project}_{species}_speciescall_{sample}.table"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk VariantsToTable -V {input.vcf} -F CHROM -F POS -F HET -O {output}"


rule index_monoclonal_vcfs:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

rule subset_monoclonal_allmaf:
###excludes polyclonal samples, as determined using RealMcCOIl
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		samplelist = config["input_lists"] + "{project}_polyclonalsamples.args",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		"gatk SelectVariants --exclude-sample-name {input.samplelist} -R {input.reference} -V {input.vcf} -O {output}"


rule index_monoclonal_vcfs_allmaf:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

rule subset_geomatched_ovale_allmaf:
###include only paired ovale samples matched by geography, still excluding polyclonal samples
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		### sample list should be a text file (.args extension) containing the names of appropriate samples for matched analyses
		samplelist = config["input_lists"] + "{project}_geomatched_ovale.args",
		reference = config["input_genomes"]+"{species}.fasta"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_geomatched_ovale_allmaf.vcf.gz"
	conda:
		"envs/biallelic.yaml"
	shell:
		#--allow-nonoverlapping-command-line-samples needed so that gatk will ignore samples in the inclusion list not in the vcf (samples of the other species, in this case)
		"gatk SelectVariants --sample-name {input.samplelist} --allow-nonoverlapping-command-line-samples -R {input.reference} -V {input.vcf} -O {output}"


rule index_geomatched_ovale_vcfs_allmaf:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_geomatched_ovale_allmaf.vcf.gz",
		#table = config["output"]+"allmaf_analysis_tables/{project}_{species}_{mixed}.table"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_geomatched_ovale_allmaf.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""


###Transfers a copy of the config file and Snakefile to the output for future reference of those results
rule copy_snakefileandconfig:
	input:
		config = "config/config.yaml",
		snakefile = "workflow/Snakefile_analysis.py"
	output:
		config = config["output"]+"pipeline/config.yaml",
		snakefile = config["output"]+"pipeline/Snakefile_analysis.py"
	shell:
		"""cp {input.config} {output.config}
		cp {input.snakefile} {output.snakefile}"""


### Step 5: Determine ratio of synonymous to nonsynonymous substitutions across the genome

###snpEff annotation files generated using the gff-based build function in snpEff
#snpEff build -gff3 -v -c /work/users/k/e/kellybce/ovale1r/snakemake/full/input/curtisi_snpeff/snpEff.config curtisigh01

####Must generate appropriate config file for each species as well

#first, add annotations to the vcf file
rule snpeff:
	input:
		vcf = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz",
		index = config["output"]+"allmaf_analysis_vcfs/{project}_{species}_{mixed}.vcf.gz.tbi",
		config = config["input"]+"{species}_snpeff/snpEff.config"
	output:
		vcf = config["output"]+"annotated_vcfs/{project}_{species}_{mixed}.vcf.gz",
		genes = config["output"]+"annotated_vcfs/{project}_{species}_{mixed}_snpeff_genes.txt",
		html = config["output"]+"annotated_vcfs/{project}_{species}_{mixed}_snpeff_summary.html",
	conda:
		"envs/snpeff.yaml"
	shell:
		"""snpEff eff -v -c {input.config} {wildcards.species} {input.vcf} > {output.vcf}
		mv snpEff_genes.txt {output.genes}
		mv snpEff_summary.html {output.html}"""


###Limit vcf to sites within the ovale 1-to-1 orthologs
rule limit_vcf_orthologues:
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_geomatched_ovale_allmaf.vcf.gz",
		index = config["output"]+"sample_sets/{project}_{species}_{mixed}_geomatched_ovale_allmaf.vcf.gz.tbi",
		bed = config["output"]+"ortholog_beds/{species}_poc-pow_orthologs_masked.bed"
	params:
		prefix = config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs"
	output:
		config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.recode.vcf"
	conda:
		"envs/masker.yaml"
	shell:
		"vcftools --gzvcf {input.vcf} --out {params.prefix} --bed {input.bed} --recode --recode-INFO-all"

###Recodes vcf limited to orthologs as a gzipped vcf
rule recode_ortholog_vcf:
	input:
		config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.recode.vcf"
	output:
		config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.vcf.gz"	
	shell:
		"bgzip -c {input} > {output}"

rule index_ortholog_vcfs_allmaf:
	input:
		vcf = config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.vcf.gz"
	output:
		config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input.vcf}"""

### Calculate dN/dS among P. ovale orthologues
rule snpeff_orthologues:
	input:
		vcf = config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.vcf.gz",
		index = config["output"]+"ortholog_vcfs/{project}_{species}_{mixed}_geomatched_ovaleorthologs.vcf.gz.tbi",
		config = config["input"]+"{species}_snpeff/snpEff.config"
	output:
		vcf = config["output"]+"annotated_ortholog_vcfs/{project}_{species}_{mixed}_ovaleorthologs.vcf.gz",
		genes = config["output"]+"annotated_ortholog_vcfs/{project}_{species}_{mixed}_ovaleorthologs_snpeff_genes.txt",
		html = config["output"]+"annotated_ortholog_vcfs/{project}_{species}_{mixed}_ovaleorthologs_snpeff_summary.html",
	conda:
		"envs/snpeff.yaml"
	shell:
		"""snpEff eff -v -c {input.config} {wildcards.species} {input.vcf} > {output.vcf}
		mv snpEff_genes.txt {output.genes}
		mv snpEff_summary.html {output.html}"""



### Step 6: Perform Principal Components Analysis using plink
rule plinker:
# generate plink files from monoclonal vcfs
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal.vcf.gz"
	output:
		#directory(config["output"]+"plink/monoclonal/{project}_{species}_{mixed}")
		config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed"
	params:
		outfile = config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}"
	conda:
		"envs/plink.yaml"
	shell:
	##--vcf-half-call haploid means that half-missing sites will be treated as haploid/homozygous, rather than missing or reference
	##--vcf-idspace-to "_" replaces spaces in sample ID names with underscores
	##--set-missing-var-ids gives new variant ID for each locus comprised of @:# -> chrom:baseposition
		'''plink --vcf {input} --make-bed --keep-allele-order --set-missing-var-ids @:# --out {params.outfile} --double-id --vcf-half-call haploid --allow-extra-chr --vcf-idspace-to "_"'''

rule ld_mark_plink:
# mark sites that are within 50 variants that exceed a linkage disequilibrium threshold
	input:
		config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed"
	output:
		config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.prune.in"
	params:
		outfile = config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		infile = config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		window = config["plink"]["window"],
		step = config["plink"]["step"],
		r2 = config["plink"]["r2"],
	conda:
		"envs/plink.yaml"	
	shell:
		'''plink --bfile {params.infile} --indep-pairwise {params.window} {params.step} {params.r2} --allow-extra-chr --out {params.outfile}'''


rule ld_filter_prune:
## selectively prune sites that are in linkage disequilibrium
	input:
		config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.prune.in"
	output:
		bed = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed",
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bim"
	params:
		infile = config["output"]+"plink/monoclonal/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		selectfile = config["output"]+"plink/ldmarked-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.prune.in",
		outfile = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}"
	conda:
		"envs/plink.yaml"
	shell:
		'''plink --bfile {params.infile} --extract {params.selectfile} --allow-extra-chr --make-bed --out {params.outfile}'''

rule pca_plink:
## generate eigenvectors and eigenvalues for all monoclonal samples in each species
	input:
		config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed"
	output:
		eigen = config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.eigenvec",
	params:
		infile = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}",
		outfile = config["output"]+"plink/pca_ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}"
	conda:
		"envs/plink.yaml"
	shell:
		'plink --bfile {params.infile} --pca "var-wts" header --allow-extra-chr --out {params.outfile}'

rule bim_alter_admixture:
###replaces non-human chromosome names so this can be read by ADMIXTURE
	input:
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bim",
	output:
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.origchrom.bim"
	params:
		tempbim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.temp.bim"
	conda:
		"envs/admixture.yaml"
	shell:
		"""cp {input.bim} {output.bim}
		awk '{{$1="0";print $0}}' {input.bim} > {params.tempbim}
		mv {params.tempbim} {input.bim}"""

rule admixture:
###for specified a priori cluster counts, assigns samples to clusters and reports cross-calidation error 10 times for each cluster count
### cross-validation error can be checked with the following bash code:
###for n in {min_cluster..max_cluster}; do for i in run*/*clusters${n}.log; do cat $i | grep "CV"; done; done
	input:
		bed = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.bed",
		#origchrom.bim only needed to ensure rule bim_alter_admixture was run previously
		bim = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/{project}_{species}_{mixed}.origchrom.bim"
	output:
		config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/run1/{project}_{species}_{mixed}.clusters{cluster}.log"
	params:
		cluster = lambda wildcards, output: wildcards.cluster,
		workdir = config["output"]+"plink/ldpruned-"+config["plink"]["window"]+"-"+config["plink"]["step"]+"-"+config["plink"]["r2"]+"/{project}_{species}_{mixed}/",
		relout = "{project}_{species}_{mixed}.clusters{cluster}.log",
		relbed = "../{project}_{species}_{mixed}.bed"
	conda:
		"envs/admixture.yaml"
	shell:
		"""cd {params.workdir};
		for k in {{1..10}}; do mkdir -p run${{k}}; cd run${{k}}; admixture --cv {params.relbed} {params.cluster} > {params.relout}; cd ..; done
		cd ../../../.."""
	


### Step 7: Calculate nucleotide diversity (pi)

## Part a: Calculate nucleotide diversity for 1-to-1-to-1 orthologs of PocGH01, PowCR01, and Pf3D7


rule unthreaded_calculate_poc_triple_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_curtisigh01_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-triple_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_triple_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-triple_{pogeneid}_masked",pogeneid = list(i[0] for i in curtisigh01_triple_orthos_masked), allow_missing =True),
		ortholist = curtisigh01_triple_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"

rule unthreaded_calculate_pow_triple_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_wallikericr01_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-triple_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_triple_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-triple_{pogeneid}_masked",pogeneid = list(i[0] for i in wallikericr01_triple_orthos_masked), allow_missing =True),
		ortholist = wallikericr01_triple_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"

rule unthreaded_calculate_pf_triple_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_pf3d7_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-triple_{geneid}_masked.windowed.pi", geneid = list(i[0] for i in pf3d7_triple_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-triple_{geneid}_masked",geneid = list(i[0] for i in pf3d7_triple_orthos_masked), allow_missing =True),
		ortholist = pf3d7_triple_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"


rule unthreaded_select_triple_pocpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-triple_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_triple_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-triple_{pogeneid}_masked_pi.txt", pogeneid = list(i[0] for i in curtisigh01_triple_orthos_masked), allow_missing =True)
	params:
		#inputpi = list(expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-triple_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing = True)),
		geneid = list(i[0] for i in curtisigh01_triple_orthos_masked),
		chrom = list(i[1] for i in curtisigh01_triple_orthos_masked),
		start = list(i[2] for i in curtisigh01_triple_orthos_masked),
		stop = list(i[3] for i in curtisigh01_triple_orthos_masked),
		window = list(i[4] for i in curtisigh01_triple_orthos_masked),
		#n_index = len(curtisigh01_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
				cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
				else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''

rule unthreaded_select_triple_powpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-triple_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_triple_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-triple_{pogeneid}_masked_pi.txt", pogeneid = list(i[0] for i in wallikericr01_triple_orthos_masked), allow_missing =True)
	params:
		#inputpi = list(expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-triple_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_triple_orthos_masked), allow_missing =True)),
		geneid = list(i[0] for i in wallikericr01_triple_orthos_masked),
		chrom = list(i[1] for i in wallikericr01_triple_orthos_masked),
		start = list(i[2] for i in wallikericr01_triple_orthos_masked),
		stop = list(i[3] for i in wallikericr01_triple_orthos_masked),
		window = list(i[4] for i in wallikericr01_triple_orthos_masked),
		#n_index = len(wallikericr01_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
			cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
			else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''

rule unthreaded_select_triple_pfpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-triple_{geneid}_masked.windowed.pi", geneid = list(i[0] for i in pf3d7_triple_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-triple_{geneid}_masked_pi.txt", geneid = list(i[0] for i in pf3d7_triple_orthos_masked), allow_missing =True)
	params:
		#inputpi = list(expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-triple_{geneid}_masked.windowed.pi", geneid = list(i[0] for i in pf3d7_triple_orthos_masked), allow_missing =True)),
		geneid = list(i[0] for i in pf3d7_triple_orthos_masked),
		chrom = list(i[1] for i in pf3d7_triple_orthos_masked),
		start = list(i[2] for i in pf3d7_triple_orthos_masked),
		stop = list(i[3] for i in pf3d7_triple_orthos_masked),
		window = list(i[4] for i in pf3d7_triple_orthos_masked),
		#n_index = len(pf3d7_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
				cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
				else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''
											
rule compile_triple_pf_pi:
###compiles all ortholog-specific pi files into a single text file containing all orthologs for a given species
	input:
		expand(config["output"]+"orthologs/stats/{project}_pf3d7_{mixed}-triple_{pfgeneid}_{masked}_pi.txt", pfgeneid=list(i[0] for i in pf3d7_triple_orthos_masked), allow_missing=True)		
	output:
		pfpi = config["output"]+"orthologs/overall/{project}_pf3d7_{mixed}_triple_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.pfpi}; done"""

rule compile_triple_poc_pi:
	input:
		expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-triple_{pocgeneid}_{masked}_pi.txt", pocgeneid=list(i[0] for i in curtisigh01_triple_orthos_masked), allow_missing=True)
	output:
		pocpi = config["output"]+"orthologs/overall/{project}_curtisigh01_{mixed}_triple_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.pocpi}; done"""

rule compile_triple_pow_pi:
	input:
		expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-triple_{powgeneid}_{masked}_pi.txt", powgeneid=list(i[0] for i in wallikericr01_triple_orthos_masked), allow_missing=True)
	output:
		powpi = config["output"]+"orthologs/overall/{project}_wallikericr01_{mixed}_triple_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.powpi}; done"""

## Part b: Calculate nucleotide diversity for 1-to-1 orthologs of PocGH01 and PowCR01
## among geographically-matched sets of P. ovale samples so that we are not biased by spatial coverage

rule unthreaded_calculate_poc_ovale_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_curtisigh01_{mixed}_geomatched_ovale_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-ovale_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_ovale_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-ovale_{pogeneid}_masked",pogeneid = list(i[0] for i in curtisigh01_ovale_orthos_masked), allow_missing =True),
		ortholist = curtisigh01_ovale_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"

rule unthreaded_calculate_pow_ovale_ortho_masked_pi:
###outputs individual summary files for each set of orthologs containing the pi of each ortholog
	input:
		vcf = config["output"]+"sample_sets/{project}_wallikericr01_{mixed}_geomatched_ovale_allmaf.vcf.gz"
	output:
		pi = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-ovale_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_ovale_orthos_masked), allow_missing =True)
	params:
		outfile = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-ovale_{pogeneid}_masked",pogeneid = list(i[0] for i in wallikericr01_ovale_orthos_masked), allow_missing =True),
		ortholist = wallikericr01_ovale_orthos_masked
	conda:
		"envs/filter.yaml"
	resources:
		mem_mb = 40000
	script:
		"scripts/ortholog_window_pi.py"


rule unthreaded_select_ovale_pocpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-ovale_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_ovale_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-ovale_{pogeneid}_masked_pi.txt", pogeneid = list(i[0] for i in curtisigh01_ovale_orthos_masked), allow_missing =True)
	params:
		#inputpi = list(expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-ovale_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in curtisigh01_orthos_masked), allow_missing = True)),
		geneid = list(i[0] for i in curtisigh01_ovale_orthos_masked),
		chrom = list(i[1] for i in curtisigh01_ovale_orthos_masked),
		start = list(i[2] for i in curtisigh01_ovale_orthos_masked),
		stop = list(i[3] for i in curtisigh01_ovale_orthos_masked),
		window = list(i[4] for i in curtisigh01_ovale_orthos_masked),
		#n_index = len(curtisigh01_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
				cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
				else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''

rule unthreaded_select_ovale_powpi:
###selects the particular windows that correspond to each ortholog's genomic window and appends to an output file for that ortholog
	input:
		pi = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-ovale_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_ovale_orthos_masked), allow_missing =True)
	output:
		pitable = expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-ovale_{pogeneid}_masked_pi.txt", pogeneid = list(i[0] for i in wallikericr01_ovale_orthos_masked), allow_missing =True)
	params:
		#inputpi = list(expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-ovale_{pogeneid}_masked.windowed.pi", pogeneid = list(i[0] for i in wallikericr01_ovale_orthos_masked), allow_missing =True)),
		geneid = list(i[0] for i in wallikericr01_ovale_orthos_masked),
		chrom = list(i[1] for i in wallikericr01_ovale_orthos_masked),
		start = list(i[2] for i in wallikericr01_ovale_orthos_masked),
		stop = list(i[3] for i in wallikericr01_ovale_orthos_masked),
		window = list(i[4] for i in wallikericr01_ovale_orthos_masked),
		#n_index = len(wallikericr01_orthos_masked)-1
	resources:
		mem_mb = 3000
	shell:
		'''input=({input.pi});
		geneid=({params.geneid});
		chrom=({params.chrom});
		start=({params.start});
		stop=({params.stop});
		window=({params.window});
		out=({output.pitable});
		echo ${{!geneid[@]}};
		echo ${{!chrom[@]}};
		for n in ${{!input[@]}}; do if cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}"; then
			cat ${{input[$n]}} | while read CHROM START END N_VARIANTS PI; do echo "${{geneid[$n]}} $CHROM $START $END $N_VARIANTS $PI"; done | grep "${{start[$n]}} ${{stop[$n]}}" >> ${{out[$n]}};
			else echo "${{geneid[$n]}} ${{chrom[$n]}} ${{start[$n]}} ${{stop[$n]}} 0 0" >> ${{out[$n]}};fi;done'''

rule compile_ovale_poc_pi:
## compiles all ortholog pi values into a single text file
	input:
		expand(config["output"]+"orthologs/stats/{project}_curtisigh01_{mixed}-ovale_{pocgeneid}_{masked}_pi.txt", pocgeneid=list(i[0] for i in curtisigh01_ovale_orthos_masked), allow_missing=True)
	output:
		pocpi = config["output"]+"orthologs/overall/{project}_curtisigh01_{mixed}_ovale_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.pocpi}; done"""

rule compile_ovale_pow_pi:
	input:
		expand(config["output"]+"orthologs/stats/{project}_wallikericr01_{mixed}-ovale_{powgeneid}_{masked}_pi.txt", powgeneid=list(i[0] for i in wallikericr01_ovale_orthos_masked), allow_missing=True)
	output:
		powpi = config["output"]+"orthologs/overall/{project}_wallikericr01_{mixed}_ovale_ortholog_{masked}_pi.txt"
	shell:
		"""for i in {input}; do cat $i >> {output.powpi}; done"""




### Step 8: Signatures of Selection

rule gunzip_vcfs:
###unzips monoclonal vcf files for use with selscan
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf.gz"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf"
	shell:
		"gunzip -c {input.vcf} > {output}"

rule remove_missing:
	###selscan requires no missing genotypes in order to impute selection, so we will filter out all sites that are missing any genotypes
	input: 
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf.vcf"
	output: 
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.recode.vcf"
	params:
		outfile = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing"
	conda: 
		"envs/filter.yaml"
	shell: 
		"vcftools --gzvcf {input} --out {params.outfile} --max-missing 1 --recode --recode-INFO-all"

rule gzip_nonmissing:
###rezips monoclonal sample set vcf file
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.recode.vcf"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz"
	shell:
		"bgzip -c {input} > {output}"

rule index_nomissing_vcfs:
###generates index for non-missing monoclonal sample vcf files
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz"
	output:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz.tbi"
	conda:
		"envs/masker.yaml"
	shell:
		"""bcftools index -t {input}"""

rule subset_chr:
###subsets vcf files to only depict specific chromosomes for use in nSl calculations
	input:
		vcf = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz",
		index = config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz.tbi",
		bed = config["input_beds"]+"{species}_chr.bed"
	output:
		config["output"]+"chr_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing_{chromosome}.vcf.gz"
	conda:
		"envs/bcftools.yaml"
	shell:
		"bcftools view -r {wildcards.chromosome} {input.vcf} | gzip > {output}"
				
rule calc_n_sl:
## calculate nSL for each species
	input:
		vcf = config["output"]+"chr_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing_{chromosome}.vcf.gz"
	output:
		config["output"]+"selection/{project}_{species}_{mixed}_nsl_{chromosome}.nsl.out"
	conda:
		"envs/selscan.yaml"
	params:
		outfile = config["output"]+"selection/{project}_{species}_{mixed}_nsl_{chromosome}"
	shell:
		"selscan --nsl --vcf {input.vcf} --maf 0.04 --out {params.outfile}"

				
rule normalize_poc_n_sl:
## normalize nSL in allele frequency bins
	input:
		nsl = expand(config["output"]+"selection/{project}_curtisigh01_{mixed}_nsl_{chromosome}.nsl.out", chromosome = curtisigh01_chr.keys(), allow_missing=True),		
	output:
		expand(config["output"]+"selection/{project}_curtisigh01_{mixed}_nsl_{chromosome}.nsl.out.100bins.norm", chromosome = curtisigh01_chr.keys(), allow_missing=True),	
	conda:
		"envs/selscan.yaml"
	params: 
	shell:
		"""norm --nsl --files {input.nsl}"""

rule normalize_pow_n_sl:
	input:
		nsl = expand(config["output"]+"selection/{project}_wallikericr01_{mixed}_nsl_{chromosome}.nsl.out", chromosome = wallikericr01_chr.keys(), allow_missing=True),		
	output:
		expand(config["output"]+"selection/{project}_wallikericr01_{mixed}_nsl_{chromosome}.nsl.out.100bins.norm", chromosome = wallikericr01_chr.keys(), allow_missing=True),	
	conda:
		"envs/selscan.yaml"
	params: 
	shell:
		"""norm --nsl --files {input.nsl}"""

rule normalize_pf_n_sl:
	input:
		nsl = expand(config["output"]+"selection/{project}_pf3d7_{mixed}_nsl_{chromosome}.nsl.out", chromosome = pf3d7_chr.keys(), allow_missing=True),		
	output:
		expand(config["output"]+"selection/{project}_pf3d7_{mixed}_nsl_{chromosome}.nsl.out.100bins.norm", chromosome = pf3d7_chr.keys(), allow_missing=True),	
	conda:
		"envs/selscan.yaml"
	params: 
	shell:
		###this command uses a default minor allele frequency filter of 0.05
		"""norm --nsl --files {input.nsl}"""


rule compile_poc_n_sl:
###compiles the chromosome, position, and n_Sl calculated at all variants into a single text file 
	input:
		poc_nsl = expand(config["output"]+"selection/{project}_curtisigh01_{mixed}_nsl_{chromosome}.nsl.out.100bins.norm", chromosome = curtisigh01_chr.keys(), allow_missing=True),
	output:
		poc_total = config["output"]+"selection/{project}_curtisigh01_{mixed}_nsl_total.txt"
	shell:
#		"""for i in {input.poc_nsl}; do cat $i | while read ID POS AF L1 L2 NSL; do echo "${{i%.nsl.out}} $POS $NSL" >> {output.poc_total}; done; done"""
		"""for i in {input.poc_nsl}; 
		do export CHROM=$(echo "$i" | cut -d "_" -f 5-6);
		cat $i | while read ID POS AF L1 L2 NSL NORM_NSL SIG; do echo "$CHROM $POS $NSL $NORM_NSL $SIG" >> {output.poc_total}; done; done;"""

rule compile_pow_n_sl:
###compiles the chromosome, position, and n_Sl calculated at all variants into a single text file 
	input:
		pow_nsl = expand(config["output"]+"selection/{project}_wallikericr01_{mixed}_nsl_{chromosome}.nsl.out.100bins.norm", chromosome = wallikericr01_chr.keys(), allow_missing=True),
	output:
		pow_total = config["output"]+"selection/{project}_wallikericr01_{mixed}_nsl_total.txt"
	shell:
		"""for i in {input.pow_nsl};
		do export CHROM=$(echo "$i" | cut -d "_" -f 5 | cut -d "." -f 1);
		cat $i | while read ID POS AF L1 L2 NSL NORM_NSL SIG; do echo "$CHROM $POS $NSL $NORM_NSL $SIG" >> {output.pow_total}; done; done;"""

rule compile_pf_n_sl:
###compiles the chromosome, position, and n_Sl calculated at all variants into a single text file 
	input:
		pf_nsl = expand(config["output"]+"selection/{project}_pf3d7_{mixed}_nsl_{chromosome}.nsl.out.100bins.norm", chromosome = pf3d7_chr.keys(), allow_missing=True),
	output:
		pf_total = config["output"]+"selection/{project}_pf3d7_{mixed}_nsl_total.txt"
	shell:
		"""for i in {input.pf_nsl};
		do export CHROM=$(echo "$i" | cut -d "_" -f 5-6);
		cat $i | while read ID POS AF L1 L2 NSL NORM_NSL SIG; do echo "$CHROM $POS $NSL $NORM_NSL $SIG" >> {output.pf_total}; done; done;"""


###Calculate Tajima's D

rule tajiima_d_sliding:
	#uses monoclonal samples only and with only non-missing sites
	#calculates nsl across entire genome using sliding windows
	input:
		config["output"]+"sample_sets/{project}_{species}_{mixed}_monoclonal_allmaf_nomissing.vcf.gz"
	output:
		config["output"]+"selection/{project}_{species}_{mixed}-"+config["tajima_window"]+".Tajima.D"
	params:
		outfile = config["output"]+"selection/{project}_{species}_{mixed}-"+config["tajima_window"],
		window = config["tajima_window"]
	conda:
		"envs/vcfkit.yaml"		
	shell:
		"vk tajima {params.window} 10 {input} > {output}"

rule select_tajima_d:
###collects tajima's D values within certain windows and compiles into two files
###one file shows the tajima's D within protein coding genes, the other does the same for exons only
### gene and exon bed files can include extrachromosomal contigs. Tajima's D values will onyl be extracted for loci with SNPs in the final analysis vcf
	input:
		stats = config["output"]+"selection/{project}_{species}_{mixed}-"+config["tajima_window"]+".Tajima.D",
		genes = config["output"]+ "beds/{species}_genes_sorted.bed",
		exons = config["output"]+ "beds/{species}_exons_sorted.bed"	
	output:
		genes = config["output"]+"selection/{project}_{species}_{mixed}_tajimad-"+config["tajima_window"]+"_genes.bed",
		exons = config["output"]+"selection/{project}_{species}_{mixed}_tajimad-"+config["tajima_window"]+"_exons.bed"
	params:
		window = config["tajima_window"]
	script:
		"scripts/gene_exon_tajima.py"
		
rule tajima_gene_search:
	#accepts table of tajiima's D in windows across genome and cross references gff file to determine the gene ID within which each Tajima's D window falls
	input:
		stats = config["output"]+"selection/{project}_{species}_{mixed}_tajimad-"+config["tajima_window"]+"_genes.bed",
		bed = config["output"]+ "beds/{species}_genes_sorted_ids.bed"
	output:
		config["output"]+"selection/{project}_{species}_{mixed}_tajimad-"+config["tajima_window"]+"_genes_IDs.bed"
	params:
		intermed = config["output"]+"selection/{project}_{species}_{mixed}_tajimad-"+config["tajima_window"]+"_genes_IDs_all.bed"
	conda:
		"envs/bedtools.yaml"
	shell:
		#First line cross-references Tajima windows with gene ID bed file to associate the Tajima's D value and gene ID for each location
		#second line removes all blank intervals (with no Tajima's D calculated, typically due to absence of SNPs)
		'''bedtools unionbedg -i {input.bed} {input.stats} > {params.intermed}
		cat {params.intermed} | while read CHROM START STOP ID TAJIMA; do if [ "$TAJIMA" != "0" ]; then echo "$CHROM $START $STOP $ID $TAJIMA" >> {output}; fi; done'''
